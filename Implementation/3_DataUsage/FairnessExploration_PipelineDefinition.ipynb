{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2f6cef7-c014-44bc-b586-02a1ada7cd35",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Pipeline definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9476705-52fb-4c04-b9be-2658d49d0132",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "- class to apply rolling means along real-valued columns of a given data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e304bc1-3eab-4e5f-a2c0-bda43e63ddb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing_RollingMean():\n",
    "        \n",
    "    def __init__(self, time_start=0, time_wind=3):\n",
    "        if not(time_wind <= time_start):\n",
    "            print('WARNING! The transform method can be applied,',\n",
    "                  'however, the mean can not be taken over time_wind={} samples'.format(time_wind+1),\n",
    "                  'for samples with indices smaller than time_wind={}.'.format(time_wind+1))\n",
    "        self.time_start = time_start\n",
    "        self.time_wind = time_wind\n",
    "    \n",
    "    def transform(self, df):\n",
    "        \n",
    "        \"\"\"\n",
    "        Compute the rolling mean along each column of df,\n",
    "        starting at the self.time_start index row and\n",
    "        using a time window of self.time_wind + 1.\n",
    "        \n",
    "        Input:\n",
    "        df:   dataframe of shape samples x sensors\n",
    "        \n",
    "        Output:\n",
    "        X:    dataframe of shape samples x sensors\n",
    "        \"\"\"\n",
    "   \n",
    "        X = []\n",
    "        index = df.loc[time_start:,].index\n",
    "        columns = df.columns\n",
    "        for idx in index:\n",
    "            x = df.loc[idx-self.time_wind:idx,:]\n",
    "            x_mean = np.mean(np.array(x), axis=0)\n",
    "            X.append(x_mean)\n",
    "        \n",
    "        X = pd.DataFrame(data=X, \n",
    "                         index=index,\n",
    "                         columns=columns)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3fb72b-30fc-4671-bfe1-7cce62538d37",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Visualization\n",
    "\n",
    "- functionality to visualize the used water distribution network, its sensor nodes and sensitive groups\n",
    "- functionality to visualize arbitrary historical data (such as training, test and predicted data) at arbitrary sensors, based on a given time index\n",
    "- functionality to visualize arbitrary historical data (such as training, test and predicted data) at arbitrary sensors, based on a given leak setting (depending on leak location and diameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e1675d-63c1-4404-871b-929e6097ec04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_network_Hanoi(node_ids,\n",
    "                       sensor_ids,\n",
    "                       df_information,\n",
    "                       wn,\n",
    "                       name='Hanoi',\n",
    "                       save_figs=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Input:\n",
    "    node_ids:         list of node labels\n",
    "    sensor_ids:       list of node labels of nodes which are sensors\n",
    "    df_information:   data frame which holds information about the settings\n",
    "    wn:               wntr water network\n",
    "    name:             string of the name of the wntr water network\n",
    "    save_figs:        boolean to determine whether creates grahps should be saved\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- find out what node belongs to which sensitive group\n",
    "    groups_per_node = dict()\n",
    "    for node_id in node_ids:\n",
    "\n",
    "        # find sensitive group of the node id\n",
    "        filter_node_id = df_information['node ID'] == node_id\n",
    "        index_node_id = list(df_information[filter_node_id].index)[0]\n",
    "        group = df_information.loc[index_node_id, 'group']\n",
    "        groups_per_node[node_id] = group\n",
    "        \n",
    "    # --- create color map  corresponding to each sensitive group\n",
    "    # (required for network plot)\n",
    "    nb_sensitive_features = len(set(groups_per_node.values()))\n",
    "    node_color_map = cm.get_cmap(name='Blues', lut=1000) #coolwarms\n",
    "    node_color_map = ListedColormap(node_color_map(np.linspace(0.3, 0.8, nb_sensitive_features)))\n",
    "\n",
    "    # --- create node attributes corresponding to each sensitive group\n",
    "    # (required for network plot)\n",
    "    node_attributes = dict()\n",
    "    node_attributes_sensors = dict()\n",
    "    for node_id in node_ids:\n",
    "        # this needs to be generalized to more or less sensitive groups\n",
    "        if groups_per_node[node_id]=='group1':\n",
    "            node_attributes[node_id] = 1/nb_sensitive_features\n",
    "            if node_id in sensor_ids:\n",
    "                node_attributes_sensors[node_id] = (0.6,0.6,0.6,0.5)\n",
    "        if groups_per_node[node_id]=='group2':\n",
    "            node_attributes[node_id] = 3/nb_sensitive_features\n",
    "            if node_id in sensor_ids:\n",
    "                node_attributes_sensors[node_id] = (0.6,0.6,0.6,0.5)\n",
    "        if groups_per_node[node_id]=='group3':\n",
    "            node_attributes[node_id] = 2/nb_sensitive_features\n",
    "            if node_id in sensor_ids:\n",
    "                node_attributes_sensors[node_id] = (0.6,0.6,0.6,0.5)\n",
    "\n",
    "    \n",
    "    # --- plot water network with sensor nodes and sensitve groups highlighted\n",
    "    fig, ax = plt.subplots(1, 1,\n",
    "                           # https://www.statology.org/subplot-size-matplotlib/\n",
    "                           figsize=(10,5))\n",
    "    ax.set_title('{} network and its sensor nodes and sensitive groups'.format(name),\n",
    "                 fontsize='medium', pad=0)\n",
    "    \n",
    "    # this is a bit un-nice because of the plot_network function plots the network automatically\n",
    "    wntr.graphics.plot_network(wn, \n",
    "                               node_attribute=node_attributes_sensors, \n",
    "                               node_cmap=node_color_map,\n",
    "                               node_size=800,\n",
    "                               node_labels=False,\n",
    "                               link_width=0,\n",
    "                               link_labels=False,\n",
    "                               add_colorbar=False,\n",
    "                               ax=ax)\n",
    "    wntr.graphics.plot_network(wn, \n",
    "                               node_attribute=node_attributes, \n",
    "                               node_cmap=node_color_map,\n",
    "                               node_size=350,\n",
    "                               node_labels=True,\n",
    "                               link_width=1,\n",
    "                               link_labels=False,\n",
    "                               add_colorbar=False,\n",
    "                               ax=ax)\n",
    "    \n",
    "    if save_figs:\n",
    "        fig.savefig('{}_sensornodes_{}_{}_{}.pdf'.format(name,\n",
    "                                                         sensor_ids[0],\n",
    "                                                         sensor_ids[1],\n",
    "                                                         sensor_ids[2]),\n",
    "                    format='pdf')\n",
    "        \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c0da12-97ed-4ccb-b2f6-c577b3acbcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_network_LTown(node_ids,\n",
    "                       sensor_ids,\n",
    "                       groups_per_node,\n",
    "                       wn,\n",
    "                       name='L-Town',\n",
    "                       save_figs=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Input:\n",
    "    node_ids:         list of node labels\n",
    "    sensor_ids:       list of node labels of nodes which are sensors\n",
    "    groups_per_node:  dictionary with key=node id, value=sensitive group\n",
    "    wn:               wntr water network\n",
    "    name:             string of the name of the wntr water network\n",
    "    save_figs:        boolean to determine whether creates grahps should be saved\n",
    "    \"\"\"\n",
    "        \n",
    "    # --- create color map  corresponding to each sensitive group\n",
    "    # (required for network plot)\n",
    "    nb_sensitive_features = len(set(groups_per_node.values()))\n",
    "    node_color_map = cm.get_cmap(name='Blues', lut=1000) #coolwarms\n",
    "    node_color_map = ListedColormap(node_color_map(np.linspace(0.3, 0.8, nb_sensitive_features)))\n",
    "\n",
    "    # --- create node attributes corresponding to each sensitive group\n",
    "    # (required for network plot)\n",
    "    node_attributes = dict()\n",
    "    node_attributes_sensors = dict()\n",
    "    for node_id in node_ids:\n",
    "        # this needs to be generalized to more or less sensitive groups\n",
    "        if groups_per_node[node_id]=='group1':\n",
    "            node_attributes[node_id] = 1/nb_sensitive_features\n",
    "            if node_id in sensor_ids:\n",
    "                node_attributes_sensors[node_id] = (0.6,0.6,0.6,0.5)\n",
    "        if groups_per_node[node_id]=='group2':\n",
    "            node_attributes[node_id] = 3/nb_sensitive_features\n",
    "            if node_id in sensor_ids:\n",
    "                node_attributes_sensors[node_id] = (0.6,0.6,0.6,0.5)\n",
    "        if groups_per_node[node_id]=='group3':\n",
    "            node_attributes[node_id] = 2/nb_sensitive_features\n",
    "            if node_id in sensor_ids:\n",
    "                node_attributes_sensors[node_id] = (0.6,0.6,0.6,0.5)\n",
    "\n",
    "    \n",
    "    # --- plot water network with sensor nodes and sensitve groups highlighted\n",
    "    fig, ax = plt.subplots(1, 1,\n",
    "                           # https://www.statology.org/subplot-size-matplotlib/\n",
    "                           figsize=(10,5))\n",
    "    ax.set_title('{} network and its sensor nodes and sensitive groups'.format(name),\n",
    "                 fontsize='medium', pad=0)\n",
    "    \n",
    "    # this is a bit un-nice because of the plot_network function plots the network automatically\n",
    "    wntr.graphics.plot_network(wn, \n",
    "                               node_attribute=node_attributes_sensors, \n",
    "                               node_cmap=node_color_map,\n",
    "                               node_size=75,\n",
    "                               node_labels=False,\n",
    "                               link_width=0,\n",
    "                               link_labels=False,\n",
    "                               add_colorbar=False,\n",
    "                               ax=ax)\n",
    "    wntr.graphics.plot_network(wn, \n",
    "                               node_attribute=node_attributes, \n",
    "                               node_cmap=node_color_map,\n",
    "                               node_size=20,\n",
    "                               node_labels=False,\n",
    "                               link_width=1,\n",
    "                               link_labels=False,\n",
    "                               add_colorbar=False,\n",
    "                               ax=ax)\n",
    "    \n",
    "    if save_figs:\n",
    "        fig.savefig('{}_sensornodes_{}_{}_{}.pdf'.format(name,\n",
    "                                                         sensor_ids[0],\n",
    "                                                         sensor_ids[1],\n",
    "                                                         sensor_ids[2]),\n",
    "                    format='pdf')\n",
    "        \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8317b4c1-b734-42a7-ae74-91d8a95434ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_data_per_timeindex(df,\n",
    "                            sensor_ids,\n",
    "                            start_ids,\n",
    "                            end_ids,\n",
    "                            thresholds=None,\n",
    "                            show_legend=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Input:\n",
    "    df:              data frame\n",
    "                     which holds the to be plotted data for each sensor\n",
    "    sensor_ids:      list of sensor ids of sensors \n",
    "                     of which the data should be visualized\n",
    "    start_ids:       list of time ids of times\n",
    "                     starting from which the data should be visualized\n",
    "    end_ids:         list of time ids of times\n",
    "                     up to which the data should be visualized\n",
    "    thresholds:      dictionary with key=sensor id, value=threshold for decision gap\n",
    "    show_legend:     boolean which indicates whether or not a legend should be shown\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- plot data for each pair of start and end IDs\n",
    "    for start_id, end_id in zip(start_ids, end_ids):\n",
    "    \n",
    "        # generate time index\n",
    "        index_plot = pd.RangeIndex(start=start_id,\n",
    "                                   stop=end_id)\n",
    "\n",
    "        # -- plot data for each sensor\n",
    "        plt.figure(figsize=(22,5))\n",
    "        color_map = cm.get_cmap(name='coolwarm', lut=100)\n",
    "        sub_color_map = color_map(np.linspace(1.0, 0.0, len(sensor_ids)))\n",
    "        for index, (color, sensor_id) in enumerate(zip(sub_color_map,sensor_ids)):\n",
    "            \n",
    "            # define linestyle depending on \n",
    "            # availability of a threshold for a decision gap and\n",
    "            # the number of grafs plotted already\n",
    "            if not(thresholds == None) and (sensor_id in thresholds.keys()):\n",
    "                    linestyle = ':'\n",
    "            else:\n",
    "                if index > 2:\n",
    "                    linestyle = '--'\n",
    "                else:\n",
    "                    linestyle = '-'\n",
    "            \n",
    "            plt.plot(index_plot, df.loc[index_plot, sensor_id], \n",
    "                     label='sensor {}'.format(sensor_id), \n",
    "                     color=color, \n",
    "                     linestyle=linestyle)\n",
    "                     #alpha=1-index/(2*len(sensor_ids)))\n",
    "\n",
    "            # plot decision gap if required information is available ...\n",
    "            if not(thresholds == None):\n",
    "                # ... and the data required is the one we consider right now\n",
    "                if sensor_id in thresholds.keys():\n",
    "                    upper_decision_gap = df.loc[index_plot, sensor_id] + thresholds[sensor_id]\n",
    "                    lower_decision_gap = df.loc[index_plot, sensor_id] - thresholds[sensor_id]\n",
    "                    plt.fill_between(index_plot, upper_decision_gap, lower_decision_gap,\n",
    "                                     label='decision gap', \n",
    "                                     color=color,\n",
    "                                     alpha=0.2)\n",
    "\n",
    "        title = 'Pressure at different sensors'\n",
    "        plt.title(title)\n",
    "        plt.xlabel('time (600s)')\n",
    "        plt.ylabel('pressure')\n",
    "        if show_legend:\n",
    "            plt.legend()\n",
    "\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135c1e1d-1816-4e10-8783-19b5c8c9d646",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_data_per_timeindex_and_sensor(dfs,\n",
    "                                       sensor_ids,\n",
    "                                       start_ids,\n",
    "                                       end_ids,\n",
    "                                       thresholds=None,\n",
    "                                       threshold_key=None,\n",
    "                                       show_legend=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Input:\n",
    "    dfs:             dictionary with key=plot label, value=data frame\n",
    "                     which holds the to be plotted data for each sensor\n",
    "    sensor_ids:      list of sensor ids of sensors \n",
    "                     of which the data should be visualized\n",
    "    start_ids:       list of time ids of times\n",
    "                     starting from which the data should be visualized\n",
    "    end_ids:         list of time ids of times\n",
    "                     up to which the data should be visualized\n",
    "    thresholds:      dictionary with key=sensor id, value=threshold for decision gap\n",
    "    threshold_key:   string which is some of dfs.keys() \n",
    "                     which indicates around which plot to draw a decision gap\n",
    "    show_legend:     boolean which indicates whether or not a legend should be shown\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- plot data for each pair of start and end IDs\n",
    "    for start_id, end_id in zip(start_ids, end_ids):\n",
    "    \n",
    "        # generate time index\n",
    "        index_plot = pd.RangeIndex(start=start_id,\n",
    "                                   stop=end_id)\n",
    "\n",
    "        # -- plot data for each sensor\n",
    "        plt.figure(figsize=(22,5))\n",
    "        for index, sensor_id in enumerate(sensor_ids):\n",
    "\n",
    "            plt.subplot(1, len(sensor_ids), index+1)\n",
    "\n",
    "            # -- plot data for each data frame in dfs\n",
    "            colors = ['grey', 'orange', 'blue']\n",
    "            linestyles = ['-', ':', ':']\n",
    "            alphas = [0.6, 1, 1]\n",
    "            for key, color, linestyle, alpha in zip(dfs, colors, linestyles, alphas):\n",
    "\n",
    "                plt.plot(index_plot, dfs[key].loc[index_plot, sensor_id], \n",
    "                         label=key, \n",
    "                         color=color, \n",
    "                         linestyle=linestyle, \n",
    "                         alpha=alpha)\n",
    "\n",
    "                # plot decision gap if required information is available ...\n",
    "                if not(thresholds == None) and not(threshold_key == None):\n",
    "                    # ... and the data required is the one we consider right now\n",
    "                    if (sensor_id in thresholds.keys()) and (key == threshold_key):\n",
    "                        upper_decision_gap = dfs[key].loc[index_plot, sensor_id] + thresholds[sensor_id]\n",
    "                        lower_decision_gap = dfs[key].loc[index_plot, sensor_id] - thresholds[sensor_id]\n",
    "                        plt.fill_between(index_plot, upper_decision_gap, lower_decision_gap,\n",
    "                                         label='decision gap', \n",
    "                                         color=color,\n",
    "                                         alpha=0.2)\n",
    "\n",
    "            title = 'Pressure at sensor {}'.format(sensor_id)\n",
    "            plt.title(title)\n",
    "            plt.xlabel('time (600s)')\n",
    "            plt.ylabel('pressure at sensor {}'.format(sensor_id))\n",
    "            if show_legend:\n",
    "                plt.legend()\n",
    "\n",
    "        plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d20ac0-9dad-4954-acb7-b9cb4c96dd42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_data_per_setting(df,\n",
    "                          df_information,\n",
    "                          sensor_ids,\n",
    "                          node_ids,\n",
    "                          diameters,\n",
    "                          setting_ids=None,\n",
    "                          thresholds=None,\n",
    "                          time_puffer=100,\n",
    "                          show_legend=True,\n",
    "                          zoom_leak=False,\n",
    "                          print_report=False): \n",
    "    \n",
    "    \"\"\"\n",
    "    Input:\n",
    "    df:              data frame\n",
    "                     which holds the to be plotted data for each sensor\n",
    "    df_information:  data frame which holds information about the settings\n",
    "    sensor_ids:      list of sensor ids of sensors \n",
    "                     of which the data should be visualized\n",
    "    node_ids:        list of node ids of nodes\n",
    "                     which belong to the settig that should be visualized\n",
    "    diameters:       list of diameters\n",
    "                     which belong to the settig that should be visualized\n",
    "    setting_ids:     list of setting ids of settings \n",
    "                     (combinations of node_id and diameter)\n",
    "                     which should be visualized\n",
    "                          if==None, setting_ids is generated by\n",
    "                          node_ids and diameters\n",
    "    thresholds:      dictionary with key=sensor id, value=threshold for decision gap\n",
    "    time_puffer:     float which indicates about how much the time index of the settings\n",
    "                     should be extended before and after the settings for visualization\n",
    "    show_legend:     boolean which indicates whether or not a legend should be shown\n",
    "    zoom_leak:       boolean which indicates whether of not the data should be zoomed\n",
    "                     to the leak in the specified settings\n",
    "    print_report:    boolean which indicates whether or not intermediate results should be printed\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- generate setting_ids (if necessary)\n",
    "    if setting_ids == None:\n",
    "        setting_ids = list()\n",
    "        for node_id in node_ids:\n",
    "            for diameter in diameters:\n",
    "                condition_node = df_information['node ID'] == node_id\n",
    "                condition_diameter = df_information['diameter'] == diameter\n",
    "                setting_id = list(df_information[condition_node & condition_diameter].index)[0]\n",
    "                setting_ids.append(setting_id)\n",
    "        if print_report:\n",
    "            print('Setting IDs where generated: {}'.format(setting_ids))\n",
    "    \n",
    "    # --- plot data for each setting\n",
    "    for setting_id in setting_ids:\n",
    "        \n",
    "        # access setting information\n",
    "        node_id = df_information.loc[setting_id,'node ID']\n",
    "        diameter = df_information.loc[setting_id,'diameter']\n",
    "        setting_start_id = df_information.loc[setting_id,'setting start ID']\n",
    "        leak_start_id = df_information.loc[setting_id,'leak start ID']\n",
    "        leak_end_id = df_information.loc[setting_id,'leak end ID']\n",
    "        setting_end_id = df_information.loc[setting_id,'setting end ID']\n",
    "        \n",
    "        # generate time index\n",
    "        if zoom_leak:\n",
    "            index_plot = pd.RangeIndex(start=leak_start_id-time_puffer,\n",
    "                                       stop=leak_end_id+time_puffer)\n",
    "        else:\n",
    "            index_plot = pd.RangeIndex(start=setting_start_id-time_puffer,\n",
    "                                       stop=setting_end_id+time_puffer)\n",
    "     \n",
    "        # -- plot data for each sensor\n",
    "        plt.figure(figsize=(22,5))\n",
    "        color_map = cm.get_cmap(name='coolwarm', lut=100)\n",
    "        sub_color_map = color_map(np.linspace(1.0, 0.0, len(sensor_ids)))\n",
    "        for index, (color, sensor_id) in enumerate(zip(sub_color_map,sensor_ids)):\n",
    "            \n",
    "            # define linestyle depending on \n",
    "            # availability of a threshold for a decision gap and\n",
    "            # the number of grafs plotted already\n",
    "            if not(thresholds == None) and (sensor_id in thresholds.keys()):\n",
    "                    linestyle = ':'\n",
    "            else:\n",
    "                if index > 2:\n",
    "                    linestyle = '--'\n",
    "                else:\n",
    "                    linestyle = '-'\n",
    "                \n",
    "            plt.plot(index_plot, df.loc[index_plot, sensor_id], \n",
    "                     label='sensor {}'.format(sensor_id), \n",
    "                     color=color, \n",
    "                     linestyle=linestyle)\n",
    "                     #alpha=1-index/(2*len(sensor_ids)))\n",
    "\n",
    "            # plot decision gap if required information is available ...\n",
    "            if not(thresholds == None): \n",
    "                # ... and the data required is the one we consider right now\n",
    "                if sensor_id in thresholds.keys():\n",
    "                    upper_decision_gap = df.loc[index_plot, sensor_id] + thresholds[sensor_id]\n",
    "                    lower_decision_gap = df.loc[index_plot, sensor_id] - thresholds[sensor_id]\n",
    "                    plt.fill_between(index_plot, upper_decision_gap, lower_decision_gap,\n",
    "                                     label='decision gap', \n",
    "                                     color=color,\n",
    "                                     alpha=0.2)\n",
    "\n",
    "            # plot start and end of leak\n",
    "            plt.scatter([leak_start_id, leak_end_id], \n",
    "                        [df.loc[leak_start_id, sensor_id],\n",
    "                         df.loc[leak_end_id, sensor_id]],\n",
    "                         label='leak start and end',\n",
    "                         color=color,\n",
    "                         marker='o')\n",
    "\n",
    "        title = 'Pressure at different sensors'\n",
    "        title += '\\nLeak location: Node {}'.format(node_id)\n",
    "        title += '\\nLeak diameter: {} cm'.format(diameter)\n",
    "        plt.title(title)\n",
    "        plt.xlabel('time (600s)')\n",
    "        plt.ylabel('pressure')\n",
    "        if show_legend:\n",
    "            plt.legend()\n",
    "\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2652e9da-6057-49db-b8e2-e1d61e4feb6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_data_per_setting_and_sensor(dfs,\n",
    "                                     df_information,\n",
    "                                     sensor_ids,\n",
    "                                     node_ids,\n",
    "                                     diameters,\n",
    "                                     setting_ids=None,\n",
    "                                     thresholds=None,\n",
    "                                     threshold_key=None,\n",
    "                                     leak_key=None,\n",
    "                                     time_puffer=100,\n",
    "                                     show_legend=True,\n",
    "                                     zoom_leak=False,\n",
    "                                     print_report=False): \n",
    "    \n",
    "    \"\"\"\n",
    "    Input:\n",
    "    dfs:             dictionary with key=plot label, value=data frame\n",
    "                     which holds the to be plotted data for each sensor\n",
    "    df_information:  data frame which holds information about the settings\n",
    "    sensor_ids:      list of sensor ids of sensors \n",
    "                     of which the data should be visualized\n",
    "    node_ids:        list of node ids of nodes\n",
    "                     which belong to the settig that should be visualized\n",
    "    diameters:       list of diameters\n",
    "                     which belong to the settig that should be visualized\n",
    "    setting_ids:     list of setting ids of settings \n",
    "                     (combinations of node_id and diameter)\n",
    "                     which should be visualized\n",
    "                          if==None, setting_ids is generated by\n",
    "                          node_ids and diameters\n",
    "    thresholds:      dictionary with key=sensor id, value=threshold for decision gap\n",
    "    threshold_key:   string which is some of dfs.keys() \n",
    "                     which indicates around which plot to draw a decision gap\n",
    "    leak_key:        string which is some of dfs.keys() \n",
    "                     which indicates on which plot to draw the leak start and end\n",
    "    time_puffer:     float which indicates about how much the time index of the settings\n",
    "                     should be extended before and after the settings for visualization\n",
    "    show_legend:     boolean which indicates whether or not a legend should be shown\n",
    "    zoom_leak:       boolean which indicates whether of not the data should be zoomed\n",
    "                     to the leak in the specified settings\n",
    "    print_report:    boolean which indicates whether or not intermediate results should be printed\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- generate setting_ids (if necessary)\n",
    "    if setting_ids == None:\n",
    "        setting_ids = list()\n",
    "        for node_id in node_ids:\n",
    "            for diameter in diameters:\n",
    "                condition_node = df_information['node ID'] == node_id\n",
    "                condition_diameter = df_information['diameter'] == diameter\n",
    "                setting_id = list(df_information[condition_node & condition_diameter].index)[0]\n",
    "                setting_ids.append(setting_id)\n",
    "        if print_report:\n",
    "            print('Setting IDs where generated: {}'.format(setting_ids))\n",
    "    \n",
    "    # --- plot data for each setting\n",
    "    for setting_id in setting_ids:\n",
    "        \n",
    "        # access setting information\n",
    "        node_id = df_information.loc[setting_id,'node ID']\n",
    "        diameter = df_information.loc[setting_id,'diameter']\n",
    "        setting_start_id = df_information.loc[setting_id,'setting start ID']\n",
    "        leak_start_id = df_information.loc[setting_id,'leak start ID']\n",
    "        leak_end_id = df_information.loc[setting_id,'leak end ID']\n",
    "        setting_end_id = df_information.loc[setting_id,'setting end ID']\n",
    "        \n",
    "        # generate time index\n",
    "        if zoom_leak:\n",
    "            index_plot = pd.RangeIndex(start=leak_start_id-time_puffer,\n",
    "                                       stop=leak_end_id+time_puffer)\n",
    "        else:\n",
    "            index_plot = pd.RangeIndex(start=setting_start_id-time_puffer,\n",
    "                                       stop=setting_end_id+time_puffer)\n",
    "     \n",
    "        # -- plot data for each sensor\n",
    "        plt.figure(figsize=(22,5))\n",
    "        for index, sensor_id in enumerate(sensor_ids):\n",
    "            \n",
    "            plt.subplot(1, len(sensor_ids), index+1)\n",
    "            \n",
    "            # -- plot data for each data frame in dfs\n",
    "            colors = ['grey', 'orange', 'blue']\n",
    "            linestyles = ['-', ':', ':']\n",
    "            alphas = [0.6, 1, 1]\n",
    "            for key, color, linestyle, alpha in zip(dfs, colors, linestyles, alphas):\n",
    "                \n",
    "                plt.plot(index_plot, dfs[key].loc[index_plot, sensor_id], \n",
    "                         label=key,\n",
    "                         color=color, \n",
    "                         linestyle=linestyle, \n",
    "                         alpha=alpha)\n",
    "            \n",
    "                # plot decision gap if required information is available ...\n",
    "                if not(thresholds == None) and not(threshold_key == None):\n",
    "                    # ... and the data required is the one we consider right now\n",
    "                    if (sensor_id in thresholds.keys()) and (key == threshold_key):\n",
    "                        upper_decision_gap = dfs[key].loc[index_plot, sensor_id] + thresholds[sensor_id]\n",
    "                        lower_decision_gap = dfs[key].loc[index_plot, sensor_id] - thresholds[sensor_id]\n",
    "                        plt.fill_between(index_plot, upper_decision_gap, lower_decision_gap,\n",
    "                                         label='decision gap', \n",
    "                                         color=color,\n",
    "                                         alpha=0.2)\n",
    "                        \n",
    "                # plot start and end of leak if required information is available ...\n",
    "                if not(leak_key == None):\n",
    "                    # ... and the data required is the one we consider right now\n",
    "                    if key == leak_key:\n",
    "                        plt.scatter([leak_start_id, leak_end_id], \n",
    "                                    [dfs[key].loc[leak_start_id, sensor_id],\n",
    "                                     dfs[key].loc[leak_end_id, sensor_id]],\n",
    "                                     label='leak start and end',\n",
    "                                     color=color,\n",
    "                                     marker='o')\n",
    "\n",
    "            title = 'Pressure at sensor {}'.format(sensor_id)\n",
    "            title += '\\nLeak location: Node {}'.format(node_id)\n",
    "            title += '\\nLeak diameter: {} cm'.format(diameter)\n",
    "            plt.title(title)\n",
    "            plt.xlabel('time (600s)')\n",
    "            plt.ylabel('pressure at sensor {}'.format(sensor_id))\n",
    "            if show_legend:\n",
    "                plt.legend()\n",
    "            \n",
    "        plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc1d2fd-6543-4d08-8cf1-32ead19cdb1c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Regression - Virtual Sensors\n",
    "\n",
    "- class to train and test a multi regression model, i.e., a regression model per column of a given data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a78923d-20ee-4d97-a248-75a739447a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiRegression():\n",
    "    \n",
    "    def __init__(self, regressor):\n",
    "        \n",
    "        \"\"\"\n",
    "        Input:\n",
    "        regressor:  regression class\n",
    "        \"\"\"\n",
    "        \n",
    "        self.regressor = regressor\n",
    "        self.regressors = dict()\n",
    "        \n",
    "    def fit(self, X_train, Y_train, print_coeff=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Train regression model (virtual sensor) per sensor node\n",
    "        based on the (pressure) inputs of the other sensor nodes.\n",
    "        \n",
    "        Input:\n",
    "        X_train:      dataframe of training (pressure) inputs \n",
    "                      of shape samples x sensors\n",
    "        Y_train:      dataframe of training (pressure) labels \n",
    "                      of shape samples x sensors\n",
    "        print_coeff:  boolean which indicates whether trained coefficients\n",
    "                      should be printed or not\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- train one regression model (virtual sensor) per sensor node\n",
    "        sensor_ids = list(X_train.columns)\n",
    "        for node in sensor_ids:\n",
    "            \n",
    "            # -- extract sensor node specific training data\n",
    "            columns = sensor_ids.copy()\n",
    "            columns.remove(node)\n",
    "            X_train_node = X_train.loc[:,columns] \n",
    "            y_train_node = Y_train.loc[:,[node]]\n",
    "            \n",
    "            # -- train regression model (virtual sensor)\n",
    "            model = self.regressor()\n",
    "            model.fit(X_train_node, y_train_node)\n",
    "            if print_coeff:\n",
    "                print('Model for sensor node {}:\\n'\\\n",
    "                      'Coef.: {}, Intercept: {}.'.format(node,\n",
    "                                                         model.coef_,\n",
    "                                                         model.intercept_))\n",
    "                \n",
    "            # -- store trained regression model (virtual sensor) \n",
    "            self.regressors[node] = dict()\n",
    "            self.regressors[node]['regressor'] = model\n",
    "            self.regressors[node]['coef_'] = model.coef_\n",
    "            self.regressors[node]['intercept_'] = model.intercept_\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        \n",
    "        \"\"\"\n",
    "        Predict (pressure) output per sensor node\n",
    "        based on the (pressure) inputs of the other sensor nodes.\n",
    "        \n",
    "        Input:\n",
    "        X_test:      dataframe of test (pressure) inputs \n",
    "                     of shape samples x sensors\n",
    "        \n",
    "        Output:\n",
    "        Y_pred:      dataframe of test (pressure) labels \n",
    "                     of shape samples x sensors\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- predict output (of virtual sensor) per sensor node\n",
    "        sensor_ids = list(X_test.columns)\n",
    "        Y_pred = pd.DataFrame(index=X_test.index,\n",
    "                              columns=X_test.columns)\n",
    "        for node in sensor_ids:\n",
    "            \n",
    "            # -- extract sensor node specific test data\n",
    "            columns = sensor_ids.copy()\n",
    "            columns.remove(node)\n",
    "            X_test_node = X_test.loc[:,columns] \n",
    "            \n",
    "            # -- access trained regression model (virtual sensor)\n",
    "            model = self.regressors[node]['regressor']\n",
    "            \n",
    "            # -- predict output (of virtual sensor)\n",
    "            y_pred = model.predict(X_test_node)\n",
    "            y_pred = pd.DataFrame(data=y_pred,\n",
    "                                  index=X_test_node.index,\n",
    "                                  columns=[node])\n",
    "            \n",
    "            # -- store regression prediction (of virtual sensors)\n",
    "            Y_pred.loc[:,node] = y_pred\n",
    "            \n",
    "        return Y_pred\n",
    "    \n",
    "    def score(self, X_test, Y_test, print_all_scores=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Compute R2 score and RMSE per sensor node.\n",
    "        \n",
    "        Input:\n",
    "        X_test:             dataframe of test (pressure) inputs \n",
    "                            of shape samples x sensors\n",
    "        Y_test:             dataframe of test (pressure) labels \n",
    "                            of shape samples x sensors\n",
    "        print_all_scores:   boolean which indicates whether scores\n",
    "                            should be printed or not\n",
    "                            \n",
    "        Output:\n",
    "        mean_r2:            mean R2 score over all sensor nodes\n",
    "                            and over all samples\n",
    "        mean_rmse:          mean RMSE over all sensor nodes\n",
    "                            over all samples\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- predict output (of virtual sensor) per sensor node\n",
    "        Y_pred = self.predict(X_test)\n",
    "        \n",
    "        # --- calculate scores per sensor node\n",
    "        sensor_ids = list(X_test.columns)\n",
    "        scores = pd.DataFrame(index=Y_test.columns,\n",
    "                              columns=['rmse','r2'])\n",
    "        for node in sensor_ids:\n",
    "            \n",
    "            # -- extract sensor node specific test and predicted data\n",
    "            y_test = Y_test.loc[:,node]\n",
    "            y_pred = Y_pred.loc[:,node]\n",
    "            \n",
    "            # -- store scores\n",
    "            scores.loc[node,'rmse'] = mean_squared_error(y_test, y_pred, \n",
    "                                                         squared=False)\n",
    "            scores.loc[node,'r2'] = r2_score(y_test, y_pred)\n",
    "        \n",
    "        if print_all_scores:\n",
    "            print('All scores:\\n', scores)\n",
    "        mean_r2 = scores.loc[:,'r2'].mean()\n",
    "        mean_rmse = scores.loc[:,'rmse'].mean()\n",
    "        \n",
    "        return mean_r2, mean_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41d3c64-1bb3-4a97-9ae2-ee54a3b5981e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Classification - Leak Dectector(s)\n",
    "\n",
    "- class to apply a threshold based classfication model that can be integrated into the multi classification resp. ensemble classification class\n",
    "- class to train and test a multi classfication model, i.e., a classification model per column of a given data frame, resp. an ensemble classification model\n",
    "- different subclasses of the multi classification resp. ensemble classification class according to different training algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0380f146-1ca7-4aa2-880c-6c7390d22d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some minor functions\n",
    "def log_on_R(x):\n",
    "    if x <= 0:\n",
    "        return -1 * np.inf\n",
    "    else:\n",
    "        return np.log(x)\n",
    "\n",
    "def sigmoid(x,b):\n",
    "    return 1/(1+np.exp(-b*x))\n",
    "\n",
    "def dx_sigmoid(x,b):\n",
    "    return b * sigmoid(x,b) * (1-sigmoid(x,b))\n",
    "\n",
    "def rates(y,f):\n",
    "    #Input:\n",
    "    #y, f:   dataframes of shape nodes x 1\n",
    "    #        and with the same column name\n",
    "    if float(y.sum()) != 0:\n",
    "        rate = (y*f).sum() / y.sum()\n",
    "        return float(rate)\n",
    "    else:\n",
    "        return '-'\n",
    "\n",
    "def TPR(y_test,y_pred):\n",
    "    #Input:\n",
    "    #y_test, y_pred:   dataframes of shape nodes x 1\n",
    "    #                  and with the same column name\n",
    "    return rates(y_test,y_pred)\n",
    "\n",
    "def FPR(y_test,y_pred):\n",
    "    return rates(1-y_test,y_pred)\n",
    "\n",
    "def FNR(y_test,y_pred):\n",
    "    return rates(y_test,1-y_pred)\n",
    "\n",
    "def TNR(y_test,y_pred):\n",
    "    return rates(1-y_test,1-y_pred)\n",
    "\n",
    "def PPV(y_test,y_pred):\n",
    "    return rates(y_pred,y_test)\n",
    "\n",
    "def ACC(y_test,y_pred):\n",
    "    acc = (y_test*y_pred).sum() + ((1-y_test)*(1-y_pred)).sum()\n",
    "    acc /= len(y_test)\n",
    "    return float(acc)\n",
    "\n",
    "def dx_ACC(y_test,dx_y_pred):\n",
    "    dx_acc = (((2*y_test)-1)*dx_y_pred).sum()\n",
    "    dx_acc /= len(y_test)\n",
    "    return float(dx_acc)\n",
    "\n",
    "def Cov(x_sen,y_pred):\n",
    "    #Input:\n",
    "    #x_sen, y_pred:   series of the shape nodes x \n",
    "    return x_sen.cov(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1c3c1e-086c-49ab-9a40-9ea3d4e41a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold classification classes \n",
    "# (based on residual inputs)\n",
    "\n",
    "class ThresholdClassification():\n",
    "    \n",
    "    def __init__(self, threshold):\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    # no fit method since this model is used in an esemble \n",
    "    # and training techniques might optimize over all esemble learner\n",
    "        \n",
    "    def predict(self, x_test):\n",
    "        \n",
    "        \"\"\"\n",
    "        Predict (leak y/n) output for one sensor node.\n",
    "        \n",
    "        Input:\n",
    "        x_test:   series of test (pressure) residual inputs\n",
    "        \n",
    "        Output:\n",
    "        y_pred:   series of test (leak y/n) outputs\n",
    "        \"\"\"\n",
    "        \n",
    "        find_leak = lambda x : 1 if (x > self.threshold) else 0\n",
    "        y_pred = x_test.apply(find_leak)\n",
    "        return y_pred\n",
    "    \n",
    "class ThresholdClassificationApproximation():\n",
    "    \n",
    "    def __init__(self,threshold):\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    # no fit method since this model is used in an esemble \n",
    "    # and training techniques might optimize over all esemble learner\n",
    "        \n",
    "    def predict(self, x_test, b_sigmoid):\n",
    "        \n",
    "        \"\"\"\n",
    "        Predict approximated (leak y/n) output for one sensor node.\n",
    "        \n",
    "        Input:\n",
    "        x_test:   series of test (pressure) residuals inputs\n",
    "        \n",
    "        Output:\n",
    "        y_pred:   series of test (leak y/n) outputs\n",
    "        \"\"\"\n",
    "        \n",
    "        y_pred = x_test.apply(lambda x: x - self.threshold)\n",
    "        y_pred = y_pred.apply(sigmoid, b=b_sigmoid)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63132f8a-91da-4495-88fc-b1614e41df8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# threshold classification ensemble super classes\n",
    "# (based on residual inputs)\n",
    "\n",
    "class EnsembleThresholdClassification():\n",
    "    \n",
    "    def __init__(self, classifier, classifier_approx):\n",
    "        \n",
    "        \"\"\"\n",
    "        Input:\n",
    "        classifier:          residual based threshold classifier class\n",
    "        classifier_approx:   residual based approximative threshold \n",
    "                             classifier class\n",
    "        \"\"\"\n",
    "        \n",
    "        self.classifier = classifier\n",
    "        self.classifier_approx = classifier_approx\n",
    "        self.thresholds = dict()\n",
    "        self.classifiers = dict()\n",
    "        self.classifiers_approx = dict()\n",
    "        \n",
    "    def fit(self):\n",
    "        print('No fitting method implemented yet')\n",
    "        \n",
    "    def fit_model(self, thresholds, print_coeff=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Train residual based threshold classification model (leak detector)\n",
    "        per sensor node\n",
    "        based on the trained thresholds in .fit() method.\n",
    "        \n",
    "        Input:\n",
    "        thresholds:    dictionary with keys:node, values:(trained) threshold\n",
    "        print_coeff:   boolean which indicates whether trained coefficients\n",
    "                       should be printed or not\n",
    "        \"\"\"\n",
    "        \n",
    "        # -- store trained classification model (leak detector) per sensor node\n",
    "        if print_coeff:\n",
    "            print('\\nFinal thresholds:')\n",
    "        for node, threshold in thresholds.items():\n",
    "            if print_coeff:\n",
    "                print('Sensor: {}, Threshold: {}.'.format(node,threshold))\n",
    "            model = self.classifier(threshold=threshold)\n",
    "            model_approx = self.classifier_approx(threshold=threshold)\n",
    "            self.classifiers[node] = dict()\n",
    "            self.classifiers[node]['classifier'] = model\n",
    "            self.classifiers[node]['threshold'] = threshold\n",
    "            self.classifiers_approx[node] = dict()\n",
    "            self.classifiers_approx[node]['classifier'] = model_approx\n",
    "            self.classifiers_approx[node]['threshold'] = threshold\n",
    "        \n",
    "    def predict(self, \n",
    "                X_test,\n",
    "                keys_list=None,\n",
    "                thresholds_array=None):\n",
    "        \n",
    "        \"\"\"\n",
    "        Predict (leak y/n) output per sensor node \n",
    "        and as an ensemble (leak y/n) output over all sensors\n",
    "        based on the (pressure) residual inputs per sensor node.\n",
    "        \n",
    "        Input:\n",
    "        X_test:             dataframe of test (pressure) residual inputs \n",
    "                            of shape samples x sensors\n",
    "        keys_list:          None or list of keys corresponding to each sensor node\n",
    "                            of shape sensors x 1\n",
    "                            and in the order of the corresponding thresholds \n",
    "                            in the thresholds_array\n",
    "                            (if None, self.thresholds.keys() is used)\n",
    "        thresholds_array:   None or numpy array of thresholds used for each sensor node\n",
    "                            of shape nodes x 1\n",
    "        \n",
    "        Output:\n",
    "        Y_pred:             dataframe of test (leak y/n) labels \n",
    "                            of shape samples x sensors\n",
    "        y_pred:             dataframe of test (leak y/n) labels\n",
    "                            of shape samples x 1\n",
    "                            \n",
    "        For training purposes, this method is used for specified \n",
    "        keys_list and thresholds_array,\n",
    "        for prediction purposes, these remain None and\n",
    "        self.thresholds \n",
    "        is automatically used instead.\n",
    "        \"\"\"\n",
    "        \n",
    "        if (keys_list!=None) and (list(thresholds_array)!=None):\n",
    "            thresholds = dict(zip(keys_list, thresholds_array))\n",
    "        else:\n",
    "            thresholds = self.thresholds\n",
    "        \n",
    "        # --- predict output (of leak detector) per sensor node\n",
    "        sensor_ids = list(X_test.columns)\n",
    "        Y_pred = pd.DataFrame(index=X_test.index,\n",
    "                              columns=X_test.columns)\n",
    "        for node in sensor_ids:\n",
    "            \n",
    "            # -- extract sensor node specific test data\n",
    "            x_test_node = X_test.loc[:,node] \n",
    "            \n",
    "            # -- access (trained) classification model (leak detector)\n",
    "            threshold = thresholds[node]\n",
    "            model = self.classifier(threshold=threshold)\n",
    "            \n",
    "            # -- predict output (of leak detector)\n",
    "            y_pred = model.predict(x_test_node)\n",
    "            \n",
    "            # -- store classification prediction (of leak detector)\n",
    "            Y_pred.loc[:,node] = y_pred\n",
    "            \n",
    "        # --- predict output (of leak detector) over all sensor nodes\n",
    "        # predict a leak if at last one classification model \n",
    "        # per node predicts a leak\n",
    "        Y_pred['sum'] =  Y_pred.agg(func='sum', axis=1)\n",
    "        find_leak = lambda su : 1 if (su >= 1) else 0\n",
    "        y_pred = Y_pred['sum'].apply(find_leak)\n",
    "        y_pred = pd.DataFrame(data=y_pred,\n",
    "                              index=y_pred.index).rename({'sum':'y'},\n",
    "                                                         axis='columns')\n",
    "                \n",
    "        return Y_pred, y_pred\n",
    "    \n",
    "    def predict_approx(self, \n",
    "                       X_test, \n",
    "                       keys_list=None,\n",
    "                       thresholds_array=None,\n",
    "                       b_sigmoid=100, \n",
    "                       sum_threshold=0.8):\n",
    "        \n",
    "        \"\"\"\n",
    "        Predict approximated (leak y/n) output per sensor node \n",
    "        and as an ensemble (leak y/n) output over all sensors\n",
    "        based on the (pressure) residual inputs per sensor node.\n",
    "        \n",
    "        Input:\n",
    "        X_test:             dataframe of test (pressure) residual inputs \n",
    "                            of shape samples x sensors\n",
    "        keys_list:          None or list of keys corresponding to each sensor node\n",
    "                            of shape sensors x 1\n",
    "                            and in the order of the corresponding thresholds \n",
    "                            in the thresholds_array\n",
    "                            (if None, self.thresholds.keys() is used)\n",
    "        thresholds_array:   None or numpy array of thresholds used for each sensor node\n",
    "                            of shape nodes x 1\n",
    "                            (if None, self.thresholds.keys() is used)\n",
    "        b_sigmoid:          positive float that indicates \n",
    "                            the b in the sigmoid function\n",
    "        sum_threshold:      positive float that indicates\n",
    "                            when the sum of outputs over all sensor nodes indicates a leak\n",
    "        \n",
    "        Output:\n",
    "        Y_pred:             dataframe of test (leak y/n) labels \n",
    "                            of shape samples x sensors\n",
    "        y_pred:             dataframe of test (leak y/n) labels\n",
    "                            of shape samples x 1\n",
    "                            \n",
    "        For training purposes, this method is used for specified \n",
    "        keys_list and thresholds_array,\n",
    "        for prediction purposes, these remain None and\n",
    "        self.thresholds \n",
    "        is automatically used instead.\n",
    "        \"\"\"\n",
    "        \n",
    "        if (keys_list!=None) and (list(thresholds_array)!=None):\n",
    "            thresholds = dict(zip(keys_list, thresholds_array))\n",
    "        else:\n",
    "            thresholds = self.thresholds\n",
    "        \n",
    "        # --- approximately predict output (of leak detector) per sensor node\n",
    "        sensor_ids = list(X_test.columns)\n",
    "        Y_pred = pd.DataFrame(index=X_test.index,\n",
    "                              columns=X_test.columns)\n",
    "        for node in sensor_ids:\n",
    "            \n",
    "            # -- extract sensor node specific test data\n",
    "            x_test_node = X_test.loc[:,node] \n",
    "            \n",
    "            # -- access (trained) approximative classification model (leak detector)\n",
    "            threshold = thresholds[node]\n",
    "            model = self.classifier_approx(threshold=threshold)\n",
    "            \n",
    "            # -- predict output (of leak detector)\n",
    "            y_pred = model.predict(x_test_node, b_sigmoid)\n",
    "            \n",
    "            # -- store classification prediction (of leak detector)\n",
    "            Y_pred.loc[:,node] = y_pred\n",
    "            \n",
    "        # --- approximately predict output (of leak detector) over all sensor nodes\n",
    "        # predict a leak if all classification models over all nodes\n",
    "        # in sum predicts a value larger than sum_thresholds\n",
    "        Y_pred['sum'] = Y_pred.agg(func='sum', axis=1)\n",
    "        y_pred = Y_pred['sum'].apply(lambda su: su - sum_threshold)\n",
    "        y_pred = y_pred.apply(sigmoid, b=b_sigmoid)\n",
    "        y_pred = pd.DataFrame(data=y_pred,\n",
    "                              index=y_pred.index).rename({'sum':'y'},\n",
    "                                                         axis='columns')\n",
    "                \n",
    "        return Y_pred, y_pred\n",
    "    \n",
    "    def visualize_predict_approx(self,\n",
    "                                 X_test,\n",
    "                                 y_test,\n",
    "                                 keys_list=None,\n",
    "                                 thresholds_array=None,\n",
    "                                 b_sigmoid=100,\n",
    "                                 sum_threshold=0.8,\n",
    "                                 iloc_index_start=0,\n",
    "                                 iloc_index_end=1000):\n",
    "        \n",
    "        \"\"\"\n",
    "        Visualize sum of non-approximated and approximated (leak y/n) outputs over all sensors\n",
    "        based on the (pressure) residual inputs per sensor node.\n",
    "        \n",
    "        Input:\n",
    "        X_test:             dataframe of test (pressure) residual inputs \n",
    "                            of shape samples x sensors\n",
    "        y_test:             dataframe of test (leak y/n) labels \n",
    "                            of shape samples x 1\n",
    "        keys_list:          List of keys corresponding to each sensor node\n",
    "                            of shape sensors x 1\n",
    "                            and in the order of the corresponding thresholds \n",
    "                            in the thresholds_array\n",
    "        thresholds_array:   Numpy array of thresholds used for each sensor node\n",
    "                            of shape nodes x 1\n",
    "        b_sigmoid:          positive float that indicates \n",
    "                            the b in the sigmoid function\n",
    "        sum_threshold:      positive float that indicates\n",
    "                            when the sum of outputs over all sensor nodes indicates a leak\n",
    "        iloc_index_start    index between 0 and len(X_test)\n",
    "        iloc_index_end      index between iloc_index_start and len(X_test)\n",
    "        \"\"\"\n",
    "\n",
    "        # --- (approximately) predict output (of leak detector) \n",
    "        # --- per sensor node and over all sensors\n",
    "        Y_pred, y_pred = self.predict(X_test,\n",
    "                                      keys_list=keys_list,\n",
    "                                      thresholds_array=thresholds_array)\n",
    "        Y_pred_a, y_pred_a = self.predict_approx(X_test,\n",
    "                                                 keys_list=keys_list,\n",
    "                                                 thresholds_array=thresholds_array,\n",
    "                                                 b_sigmoid=b_sigmoid, \n",
    "                                                 sum_threshold=sum_threshold)\n",
    "\n",
    "        # --- plot true labels, \n",
    "        # --- sum of (approximated) predicted outputs over all sensor nodes and\n",
    "        # --- sum threshold\n",
    "        nb_sensors = len(Y_pred.columns)-1\n",
    "        \n",
    "        # access true labels, (approximated) predictions and sum threshold\n",
    "        Y = Y_pred.sort_index()\n",
    "        Y = Y.iloc[iloc_index_start:iloc_index_end,:]\n",
    "        Y_a = Y_pred_a.sort_index()\n",
    "        Y_a = Y_a.iloc[iloc_index_start:iloc_index_end,:]\n",
    "        y = y_clas_train.sort_index()\n",
    "        y = y.iloc[iloc_index_start:iloc_index_end,:]\n",
    "        t = [1 for y in range(len(y))]\n",
    "        t_a = [sum_threshold for y in range(len(y))]\n",
    "        \n",
    "        # true labels vs. sum of predicted outputs\n",
    "        plt.figure(figsize=(20,10))\n",
    "        plt.scatter(list(range(len(Y))), Y['sum'])\n",
    "        plt.scatter(list(range(len(y))), y*nb_sensors, marker='.')\n",
    "        plt.plot(list(range(len(y))), t, color='red', linestyle='--')\n",
    "        plt.title('True labels vs. sum of predicted outputs')\n",
    "        plt.show()\n",
    "\n",
    "        # true labels vs. sum of (approximated) predicted outputs\n",
    "        plt.figure(figsize=(20,10))\n",
    "        plt.scatter(list(range(len(Y_a))), Y_a['sum'])\n",
    "        plt.scatter(list(range(len(y))), y*nb_sensors, marker='.')\n",
    "        plt.plot(list(range(len(y))), t_a, color='red', linestyle='--')\n",
    "        plt.title('True labels vs. sum of approximated predicted outputs')\n",
    "        plt.show()\n",
    "        \n",
    "        # --- evaluate (approximated) predictions\n",
    "        tpr = TPR(y_test,y_pred)\n",
    "        fpr = FPR(y_test,y_pred)\n",
    "        acc = ACC(y_test,y_pred)\n",
    "\n",
    "        print('Exact TPR:', tpr)\n",
    "        print('Exact FPR:', fpr)\n",
    "        print('Exact ACC:', acc)\n",
    "        \n",
    "        tpr = TPR(y_test,y_pred_a)\n",
    "        fpr = FPR(y_test,y_pred_a)\n",
    "        acc = ACC(y_test,y_pred_a)\n",
    "\n",
    "        print('Approximated TPR:', tpr)\n",
    "        print('Approximated FPR:', fpr)\n",
    "        print('Approximated ACC:', acc)\n",
    "    \n",
    "    def dx_predict_approx(self, \n",
    "                          X_test, \n",
    "                          keys_list=None,\n",
    "                          thresholds_array=None,\n",
    "                          b_sigmoid=100, \n",
    "                          sum_threshold=0.8):\n",
    "        \n",
    "        \"\"\"\n",
    "        Compute gradient with respect to thresholds per sensor node\n",
    "        of approximated (leak y/n) output prediction per sensor node \n",
    "        based on the (pressure) residual inputs per sendor node.\n",
    "        \n",
    "        Input:\n",
    "        X_test:             dataframe of test (pressure) residual inputs \n",
    "                            of shape samples x sensors\n",
    "        keys_list:          None or list of keys corresponding to each sensor node\n",
    "                            of shape sensors x 1\n",
    "                            and in the order of the corresponding thresholds \n",
    "                            in the thresholds_array\n",
    "                            (if None, self.thresholds.keys() is used)\n",
    "        thresholds_array:   None or numpy array of thresholds used for each sensor node\n",
    "                            of shape nodes x 1\n",
    "                            (if None, self.thresholds.keys() is used)\n",
    "        b_sigmoid:          positive float that indicates \n",
    "                            the b in the sigmoid function\n",
    "        sum_threshold:      positive float that indicates\n",
    "                            when the sum of sensor node outputs indicates a leak\n",
    "        \n",
    "        Output:\n",
    "        dx_Y_pred:          dataframe of partial derivatives of the\n",
    "                            test (leak y/n) labels \n",
    "                            with respect to the thresholds per sensor node\n",
    "                            of shape samples x sensors\n",
    "                           \n",
    "        For training purposes, this method is used for specified \n",
    "        keys_list and thresholds_array,\n",
    "        for prediction purposes, these remain None and\n",
    "        self.thresholds \n",
    "        is automatically used instead.\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- approximately predict output (of leak detector) \n",
    "        # --- per sensor node and over all sensors\n",
    "        Y_pred, y_pred = self.predict_approx(X_test,\n",
    "                                             keys_list=keys_list,\n",
    "                                             thresholds_array=thresholds_array,\n",
    "                                             b_sigmoid=b_sigmoid, \n",
    "                                             sum_threshold=sum_threshold)\n",
    "        \n",
    "        # --- compute partial derivatives of approximately predicted output \n",
    "        # --- wrt. each sensor node's threshold\n",
    "        # -- precompute factors which are the same for each partial derivative\n",
    "        # (this is the \"outer\" derivative)\n",
    "        factor = - b_sigmoid**2 * y_pred * (1 - y_pred)\n",
    "        \n",
    "        # -- precompute factors which are sensor specific\n",
    "        # (this is the \"inner\" derivative)\n",
    "        factor_nodes = Y_pred * (1 - Y_pred)\n",
    "        \n",
    "        # -- combine outer and inner derivative per sensor node\n",
    "        sensor_ids = list(X_test.columns)\n",
    "        dx_Y_pred = pd.DataFrame(index=X_test.index,\n",
    "                                 columns=X_test.columns)\n",
    "        for node in sensor_ids:\n",
    "            \n",
    "            # rename first factor to make pandas series multiplication possible\n",
    "            factor_node_1 = factor.rename({'y':node},\n",
    "                                          axis='columns')\n",
    "            # access inner derivative of approximately predicted output \n",
    "            # wrt. to sensors's threshold\n",
    "            factor_node_2 = factor_nodes.loc[:,[node]]\n",
    "            \n",
    "            # store partial derivatives of approximately predicted output \n",
    "            dx_Y_pred.loc[:,node] = factor_node_1 * factor_node_2\n",
    "            \n",
    "        return dx_Y_pred\n",
    "            \n",
    "    def score(self, \n",
    "              X_test, \n",
    "              y_test,\n",
    "              keys_list=None,\n",
    "              thresholds_array=None,\n",
    "              print_all_scores=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Compute TPR, FNR, FPR, TNR, PPV and relative amout of positive predictions\n",
    "        with respect to ensemble (leak y/n) output over all sensors\n",
    "        based on the (pressure) residual inputs per sensor node.\n",
    "        \n",
    "        Input:\n",
    "        X_test:             dataframe of test (pressure) residual inputs \n",
    "                            of shape samples x sensors\n",
    "        y_test:             dataframe of test (leak y/n) labels \n",
    "                            of shape samples x 1\n",
    "        keys_list:          None or list of keys corresponding to each sensor node\n",
    "                            of shape sensors x 1\n",
    "                            and in the order of the corresponding thresholds \n",
    "                            in the thresholds_array\n",
    "                            (if None, self.thresholds.keys() is used)\n",
    "        thresholds_array:   None or numpy array of thresholds used for each sensor node\n",
    "                            of shape nodes x 1\n",
    "        print_all_scores:   boolean which indicates whether scores\n",
    "                            should be printed or not\n",
    "                            \n",
    "        Output:\n",
    "        tpr:                float of TPR over all samples\n",
    "        fnr:                float of FNR over all samples\n",
    "        fpr:                float of FPR over all samples\n",
    "        tnr:                float of TNR over all samples\n",
    "        ppv:                float of PPV score over all samples\n",
    "        acc:                float of accuracy over all samples\n",
    "        pos_pred:           float of amount of positive predictions \n",
    "                            over all samples\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- predict output (of leak detector) over all sensor nodes\n",
    "        _, y_pred = self.predict(X_test,\n",
    "                                 keys_list=keys_list,\n",
    "                                 thresholds_array=thresholds_array)\n",
    "        \n",
    "        # --- calculate scores\n",
    "        tpr = TPR(y_test,y_pred) # tp/(tp+fn)\n",
    "        fnr = FNR(y_test,y_pred) # fn/(tp+fn)\n",
    "        fpr = FPR(y_test,y_pred) # fp/(fp+tn)\n",
    "        tnr = TNR(y_test,y_pred) # tn/(fp+tn)\n",
    "        ppv = PPV(y_test,y_pred) # tp/(tp+fp)\n",
    "        acc = ACC(y_test,y_pred) # (tp+tn)/(tp+fn+fp+tn)\n",
    "        pos_pred = float(y_pred.sum())/len(y_pred) # (tp+fp)/(tp+fn+fp+tn)\n",
    "        \n",
    "        if print_all_scores:\n",
    "            print('TPR:', tpr)\n",
    "            print('FNR = 1 - TPR:', fnr)\n",
    "            print('FPR:', fpr)\n",
    "            print('TNR = 1 - FPR:', tnr)\n",
    "            print('PPV:', ppv)\n",
    "            print('Acc.:', acc)\n",
    "            print('Rel. pos. predictions:', pos_pred)\n",
    "            print('TPR - FPR:', tpr-fpr)\n",
    "        \n",
    "        return tpr, fnr, fpr, tnr, ppv, acc, pos_pred\n",
    "    \n",
    "    def score_fairness(self, \n",
    "                       X_test, \n",
    "                       X_sen_test, \n",
    "                       y_test, \n",
    "                       keys_list=None,\n",
    "                       thresholds_array=None,\n",
    "                       print_all_scores=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Compute TPR, equal opportunity and disparate impact\n",
    "        with respect to ensemble (leak y/n) output over all sensors\n",
    "        based on the (pressure) residual inputs per sensor node.\n",
    "        \n",
    "        Input:\n",
    "        X_test:             dataframe of test (pressure) residual inputs \n",
    "                            of shape samples x sensors\n",
    "        X_sen_test:         dataframe of test sensitive (feature) inputs \n",
    "                            of shape samples x sensitive features\n",
    "        y_test:             dataframe of test (leak y/n) labels \n",
    "                            of shape samples x 1\n",
    "        keys_list:          None or list of keys corresponding to each sensor node\n",
    "                            of shape sensors x 1\n",
    "                            and in the order of the corresponding thresholds \n",
    "                            in the thresholds_array\n",
    "                            (if None, self.thresholds.keys() is used)\n",
    "        thresholds_array:   None or numpy array of thresholds used for each sensor node\n",
    "                            of shape nodes x 1\n",
    "        print_all_scores:   boolean which indicates whether scores\n",
    "                            should be printed or not\n",
    "                            \n",
    "        Output:\n",
    "        TPRs_list:          dict with key:sensitive group, \n",
    "                            value:TPR over all samples \n",
    "        eo:                 float of equal opportunity score\n",
    "        di:                 float of disparate impact score\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- compute TPR per sensitive group\n",
    "        sensitive_features = list(X_sen_test.columns)\n",
    "        TPRs_dict = dict()\n",
    "        for sensitive_feature in sensitive_features:\n",
    "\n",
    "            filter_group = X_sen_test[sensitive_feature] == 1\n",
    "            tpr,_,_,_,_,_,_ = self.score(X_test[filter_group], \n",
    "                                         y_test[filter_group],\n",
    "                                         keys_list=keys_list,\n",
    "                                         thresholds_array=thresholds_array)\n",
    "            TPRs_dict[sensitive_feature] = tpr\n",
    "\n",
    "        # --- compute fairness scores\n",
    "        TPRs_list = list(TPRs_dict.values())\n",
    "        eo = max(TPRs_list) - min(TPRs_list)\n",
    "        di = min(TPRs_list)/max(TPRs_list)\n",
    "\n",
    "        if print_all_scores:\n",
    "            print('TPRs:\\n', TPRs_dict)\n",
    "            print('Equal opportunity score (< epsilon), e.g. < 0.2:\\n', eo)\n",
    "            print('Disparate impact score (> 1 - epsilon), e.g. > 0.8:\\n', di)\n",
    "            # note that (1 - di) * max(TPRs_list) == eo\n",
    "\n",
    "        return TPRs_dict, eo, di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac822a0e-f275-4a2f-95c9-f22e82622cc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# threshold classification ensemble sub classes\n",
    "# (based on residual inputs)\n",
    "class ETC_hyperparameter(EnsembleThresholdClassification):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 alias='H',\n",
    "                 classifier=ThresholdClassification, \n",
    "                 classifier_approx=ThresholdClassificationApproximation):\n",
    "        super().__init__(classifier, classifier_approx)\n",
    "        self.alias = alias\n",
    "        \n",
    "    def fit(self, \n",
    "            X_train, \n",
    "            factor=1, \n",
    "            print_coeff=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Train residual based threshold classification model (leak detector)\n",
    "        per sensor node\n",
    "        based on the (pressure) residual inputs per sensor nodes\n",
    "        by learning one threshold per sensor node\n",
    "        by choosing some procentual amount of the largest training error \n",
    "        per sensor node.\n",
    "        \n",
    "        Input:\n",
    "        X_train:      dataframe of training (pressure) residual inputs \n",
    "                      of shape samples x sensors\n",
    "        factor:       positive float that is the procentual amount \n",
    "                      of the largest training error that should be used\n",
    "        print_coeff:  boolean which indicates whether trained coefficients\n",
    "                      should be printed or not\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- train one classification model (leak detector) per sensor node\n",
    "        # --- by finding the largest training error per node\n",
    "        sensor_ids = list(X_train.columns)\n",
    "        for node in sensor_ids:\n",
    "\n",
    "            # -- train classification model (leak detector)\n",
    "            X_train_node = X_train.loc[:,node]\n",
    "            threshold = factor * X_train_node.max()\n",
    "            self.thresholds[node] = threshold\n",
    "            \n",
    "        # -- store trained classification model (leak detector) per sensor node\n",
    "        self.fit_model(self.thresholds, print_coeff=print_coeff)\n",
    "            \n",
    "class ETC_optimizeFTPR_db(EnsembleThresholdClassification):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 alias='T-F-PR',\n",
    "                 classifier=ThresholdClassification, \n",
    "                 classifier_approx=ThresholdClassificationApproximation):\n",
    "        super().__init__(classifier, classifier_approx)\n",
    "        self.alias = alias\n",
    "            \n",
    "    def fit(self, \n",
    "            X_train,\n",
    "            X_sen_train,\n",
    "            y_train,\n",
    "            start_thresholds,\n",
    "            b_sigmoid=100, \n",
    "            sum_threshold=0.8,\n",
    "            print_coeff=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Train residual based threshold classification model (leak detector) \n",
    "        per sensor node\n",
    "        based on the (pressure) residual inputs per sensor nodes\n",
    "        by learning one threshold per sensor node\n",
    "        by minimizing the db. approximation of the objective F = -TPR + FPR \n",
    "        over these thresholds.\n",
    "        \n",
    "        Input:\n",
    "        X_train:            dataframe of training (pressure) residual inputs \n",
    "                            of shape samples x sensors\n",
    "        X_sen_train         dataframe of training sensitive (feature) inputs \n",
    "                            of shape samples x sensitive features\n",
    "                            (only required for evaluation if print_coeff=True)\n",
    "        y_train:            dataframe of training (leak y/n) labels \n",
    "                            of shape samples x 1\n",
    "        start_thresholds:   numpy array of thresholds used for each sensor node\n",
    "                            of shape nodes x 1\n",
    "        b_sigmoid:          positive float that indicates \n",
    "                            the b in the sigmoid function\n",
    "        sum_thresholds:     positive float that indicates\n",
    "                            when the sum of sensor node outputs indicates a leak\n",
    "        print_coeff:        boolean which indicates whether trained coefficients\n",
    "                            should be printed or not\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- train one classification model (leak detector) per sensor node\n",
    "        # --- by minimizing an objective of the ensemble classification model\n",
    "        \n",
    "        # -- define objective and objective's gradient\n",
    "        def F(thresholds_array):\n",
    "            _, y_pred = self.predict_approx(X_train, \n",
    "                                            keys_list=list(start_thresholds.keys()),\n",
    "                                            thresholds_array=thresholds_array, \n",
    "                                            b_sigmoid=b_sigmoid,\n",
    "                                            sum_threshold=sum_threshold)\n",
    "            F = -TPR(y_train,y_pred) + FPR(y_train,y_pred)\n",
    "            \n",
    "            if print_coeff:\n",
    "                print('\\n\\nCurrent thresholds:')\n",
    "                for threshold in thresholds_array:\n",
    "                    print(threshold)\n",
    "                \n",
    "                print('\\nCurrent objective information:')\n",
    "                print('TPR:', TPR(y_train,y_pred))\n",
    "                print('FPR:', FPR(y_train,y_pred))\n",
    "                print('F:  ', F)\n",
    "                \n",
    "                print('\\nCurrent fairness information:')\n",
    "                sensitive_features = list(X_sen_train.columns)\n",
    "                for sensitive_feature in sensitive_features:\n",
    "                    x_sen_train = X_sen_train.loc[:, sensitive_feature]\n",
    "                    cov = Cov(x_sen_train, y_pred.loc[:,'y'])\n",
    "                    print('Group:', sensitive_feature)\n",
    "                    print('  cov:', cov)\n",
    "        \n",
    "                print('\\nCurrent performance information:')\n",
    "                tpr,_,fpr,_,_,acc,_ = self.score(X_train,\n",
    "                                                 y_train,\n",
    "                                                 keys_list=list(start_thresholds.keys()),\n",
    "                                                 thresholds_array=thresholds_array)\n",
    "                _,_,di = self.score_fairness(X_train,\n",
    "                                             X_sen_train,\n",
    "                                             y_train,\n",
    "                                             keys_list=list(start_thresholds.keys()),\n",
    "                                             thresholds_array=thresholds_array)\n",
    "                print('TPR, FPR, ACC, DI:', tpr, fpr, acc, di)\n",
    "                \n",
    "            return np.array(F)\n",
    "            \n",
    "        def gradF(thresholds_array):\n",
    "            dx_Y_pred = self.dx_predict_approx(X_train,\n",
    "                                               keys_list=list(start_thresholds.keys()),\n",
    "                                               thresholds_array=thresholds_array, \n",
    "                                               b_sigmoid=b_sigmoid,\n",
    "                                               sum_threshold=sum_threshold)\n",
    "            gradF = list()\n",
    "            sensor_ids = list(X_train.columns)\n",
    "            for node in sensor_ids:\n",
    "                dx_y_pred = dx_Y_pred.loc[:,node]\n",
    "                dx_y_pred = pd.DataFrame(data=dx_y_pred,\n",
    "                                         index=dx_y_pred.index)\n",
    "                dx_y_pred = dx_y_pred.rename({node:'y'},\n",
    "                                              axis='columns')\n",
    "                partialF = -TPR(y_train,dx_y_pred) + FPR(y_train,dx_y_pred)\n",
    "                gradF.append(partialF)\n",
    "                \n",
    "            if print_coeff:\n",
    "                print('\\ngradF:', gradF)\n",
    "                \n",
    "            return np.array(gradF)\n",
    "        \n",
    "        # -- run minimization of objective\n",
    "        start_thresholds_array = list(start_thresholds.values())\n",
    "        start = time.time()\n",
    "        result = minimize(fun=F, \n",
    "                          x0=start_thresholds_array, \n",
    "                          jac=gradF, \n",
    "                          method=\"BFGS\")\n",
    "        end = time.time()\n",
    "        print('\\n{} (Time: {} sec.)'.format(result.message, end - start))\n",
    "        thresholds_array = result.x\n",
    "        self.thresholds = dict(zip(start_thresholds.keys(), thresholds_array))\n",
    "    \n",
    "        # -- store trained classification model (leak detector) per sensor node\n",
    "        self.fit_model(self.thresholds, print_coeff=print_coeff)\n",
    "            \n",
    "            \n",
    "class ETC_optimizeFTPR_F_db(EnsembleThresholdClassification):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 alias='T-F-PR+F',\n",
    "                 classifier=ThresholdClassification, \n",
    "                 classifier_approx=ThresholdClassificationApproximation):\n",
    "        super().__init__(classifier, classifier_approx)\n",
    "        self.alias = alias\n",
    "            \n",
    "    def fit(self, \n",
    "            X_train,\n",
    "            X_sen_train,\n",
    "            y_train,\n",
    "            start_thresholds,\n",
    "            b_sigmoid=100, \n",
    "            sum_threshold=0.8,\n",
    "            c=0.5,\n",
    "            mu=0.1,\n",
    "            print_coeff=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Train residual based threshold classification model (leak detector) \n",
    "        per sensor node\n",
    "        based on the (pressure) residual inputs per sensor nodes\n",
    "        by learning one threshold per sensor node\n",
    "        by minimizing the db. approximation of the objective F = -TPR + FPR \n",
    "        over these thresholds.\n",
    "        under some db. approximation covariance based side constraints, \n",
    "        put into a log barrier penalty.\n",
    "        \n",
    "        Input:\n",
    "        X_train:            dataframe of training (pressure) residual inputs \n",
    "                            of shape samples x sensors\n",
    "        X_sen_train         dataframe of training sensitive (feature) inputs \n",
    "                            of shape samples x sensitive features\n",
    "        y_train:            dataframe of training (leak y/n) labels \n",
    "                            of shape samples x 1\n",
    "        start_thresholds:   numpy array of thresholds used for each sensor node\n",
    "                            of shape nodes x 1\n",
    "        b_sigmoid:          positive float that indicates \n",
    "                            the b in the sigmoid function\n",
    "        sum_thresholds:     positive float that indicates\n",
    "                            when the sum of sensor node outputs indicates a leak\n",
    "        c:                  positive float that indicates\n",
    "                            the upper bound of the absolute value\n",
    "                            of the side constraint(s)\n",
    "        mu:                 positive float that indicates\n",
    "                            the hyperparameter of the log barrier method                    \n",
    "        print_coeff:        boolean which indicates whether trained coefficients\n",
    "                            should be printed or not\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- train one classification model (leak detector) per sensor node\n",
    "        # --- by minimizing an objective of the ensemble classification model\n",
    "        \n",
    "        # -- define objective and objective's gradient\n",
    "        def F(thresholds_array):\n",
    "            _, y_pred = self.predict_approx(X_train, \n",
    "                                            keys_list=list(start_thresholds.keys()),\n",
    "                                            thresholds_array=thresholds_array, \n",
    "                                            b_sigmoid=b_sigmoid,\n",
    "                                            sum_threshold=sum_threshold)\n",
    "            F = -TPR(y_train,y_pred) + FPR(y_train,y_pred)\n",
    "            \n",
    "            if print_coeff:\n",
    "                print('\\n\\nCurrent thresholds:')\n",
    "                for threshold in thresholds_array:\n",
    "                    print(threshold)\n",
    "                \n",
    "                print('\\nCurrent objective and fairness information:')\n",
    "                print('TPR:', TPR(y_train,y_pred))\n",
    "                print('FPR:', FPR(y_train,y_pred))\n",
    "                print('F:  ', F)\n",
    "            \n",
    "            sensitive_features = list(X_sen_train.columns)\n",
    "            for sensitive_feature in sensitive_features:\n",
    "                x_sen_train = X_sen_train.loc[:, sensitive_feature]\n",
    "                cov = Cov(x_sen_train, y_pred.loc[:,'y'])\n",
    "                F -= mu * log_on_R(c - cov)\n",
    "                F -= mu * log_on_R(c + cov)\n",
    "                \n",
    "                if print_coeff:\n",
    "                    print('Group:', sensitive_feature)\n",
    "                    print('  cov:', cov)\n",
    "                    print('  - mu * log_on_R(c - cov):', - mu * log_on_R(c - cov))\n",
    "                    print('  - mu * log_on_R(c + cov):', - mu * log_on_R(c + cov))\n",
    "                    print('  F:', F)\n",
    "                    \n",
    "            if print_coeff:\n",
    "                print('\\nCurrent performance information:')\n",
    "                tpr,_,fpr,_,_,acc,_ = self.score(X_train,\n",
    "                                                 y_train,\n",
    "                                                 keys_list=list(start_thresholds.keys()),\n",
    "                                                 thresholds_array=thresholds_array)\n",
    "                _,_,di = self.score_fairness(X_train,\n",
    "                                             X_sen_train,\n",
    "                                             y_train,\n",
    "                                             keys_list=list(start_thresholds.keys()),\n",
    "                                             thresholds_array=thresholds_array)\n",
    "                print('TPR, FPR, ACC, DI:', tpr, fpr, acc, di)\n",
    "            \n",
    "            return np.array(F)\n",
    "            \n",
    "        def gradF(thresholds_array):\n",
    "            _, y_pred = self.predict_approx(X_train, \n",
    "                                            keys_list=list(start_thresholds.keys()),\n",
    "                                            thresholds_array=thresholds_array, \n",
    "                                            b_sigmoid=b_sigmoid,\n",
    "                                            sum_threshold=sum_threshold)\n",
    "            dx_Y_pred = self.dx_predict_approx(X_train,\n",
    "                                               keys_list=list(start_thresholds.keys()),\n",
    "                                               thresholds_array=thresholds_array, \n",
    "                                               b_sigmoid=b_sigmoid,\n",
    "                                               sum_threshold=sum_threshold)\n",
    "            gradF = list()\n",
    "            sensor_ids = list(X_train.columns)\n",
    "            for node in sensor_ids:\n",
    "                dx_y_pred = dx_Y_pred.loc[:,node]\n",
    "                dx_y_pred = pd.DataFrame(data=dx_y_pred,\n",
    "                                         index=dx_y_pred.index)\n",
    "                dx_y_pred = dx_y_pred.rename({node:'y'},\n",
    "                                              axis='columns')\n",
    "                partialF = -TPR(y_train,dx_y_pred) + FPR(y_train,dx_y_pred)\n",
    "                \n",
    "                sensitive_features = list(X_sen_train.columns)\n",
    "                for sensitive_feature in sensitive_features:\n",
    "                    x_sen_train = X_sen_train.loc[:, sensitive_feature]\n",
    "                    cov = Cov(x_sen_train, y_pred.loc[:,'y'])\n",
    "                    dx_cov = Cov(x_sen_train, dx_y_pred.loc[:,'y'])\n",
    "                    partialF -= (mu * -1 * dx_cov) / (c - cov)\n",
    "                    partialF -= (mu * dx_cov) / (c + cov)\n",
    "                gradF.append(partialF)\n",
    "                \n",
    "            if print_coeff:\n",
    "                print('\\ngradF:', gradF)\n",
    "                \n",
    "            return np.array(gradF)\n",
    "        \n",
    "        # -- run minimization of objective\n",
    "        start_thresholds_array = list(start_thresholds.values())\n",
    "        start = time.time()\n",
    "        result = minimize(fun=F, \n",
    "                          x0=start_thresholds_array, \n",
    "                          jac=gradF, \n",
    "                          method=\"BFGS\")\n",
    "        end = time.time()\n",
    "        print('\\n{} (Time: {} sec.)'.format(result.message, end - start))\n",
    "        thresholds_array = result.x\n",
    "        self.thresholds = dict(zip(start_thresholds.keys(), thresholds_array))\n",
    "    \n",
    "        # -- store trained classification model (leak detector) per sensor node\n",
    "        self.fit_model(self.thresholds, print_coeff=print_coeff)\n",
    "        \n",
    "class ETC_optimizeFTPR_ndb(EnsembleThresholdClassification):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 alias='T-F-PR-ndb',\n",
    "                 classifier=ThresholdClassification, \n",
    "                 classifier_approx=ThresholdClassificationApproximation):\n",
    "        super().__init__(classifier, classifier_approx)\n",
    "        self.alias = alias\n",
    "            \n",
    "    def fit(self, \n",
    "            X_train, \n",
    "            X_sen_train,\n",
    "            y_train,\n",
    "            start_thresholds,\n",
    "            print_coeff=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Train residual based threshold classification model (leak detector) \n",
    "        per sensor node\n",
    "        based on the (pressure) residual inputs per sensor nodes\n",
    "        by learning one threshold per sensor node\n",
    "        by minimizing the exact objective F = -TPR + FPR \n",
    "        over these thresholds.\n",
    "        \n",
    "        Input:\n",
    "        X_train:            dataframe of training (pressure) residual inputs \n",
    "                            of shape samples x sensors\n",
    "        X_sen_train         dataframe of training sensitive (feature) inputs \n",
    "                            of shape samples x sensitive features\n",
    "                            (only required for evaluation if print_coeff=True)\n",
    "        y_train:            dataframe of training (leak y/n) labels \n",
    "                            of shape samples x 1\n",
    "        start_thresholds:   numpy array of thresholds used for each sensor node\n",
    "                            of shape nodes x 1\n",
    "        print_coeff:        boolean which indicates whether trained coefficients\n",
    "                            should be printed or not\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- train one classification model (leak detector) per sensor node\n",
    "        # --- by minimizing an objective of the ensemble classification model\n",
    "        \n",
    "        # -- define objective \n",
    "        def F(thresholds_array):\n",
    "            _, y_pred = self.predict(X_train, \n",
    "                                     keys_list=list(start_thresholds.keys()),\n",
    "                                     thresholds_array=thresholds_array)\n",
    "            F = -TPR(y_train,y_pred) + FPR(y_train,y_pred)\n",
    "            \n",
    "            if print_coeff:\n",
    "                print('\\n\\nCurrent thresholds:')\n",
    "                for threshold in thresholds_array:\n",
    "                    print(threshold)\n",
    "                \n",
    "                print('\\nCurrent objective information:')\n",
    "                print('TPR:', TPR(y_train,y_pred))\n",
    "                print('FPR:', FPR(y_train,y_pred))\n",
    "                print('F:  ', F)\n",
    "                \n",
    "                print('\\nCurrent fairness information:')\n",
    "                sensitive_features = list(X_sen_train.columns)\n",
    "                for sensitive_feature in sensitive_features:\n",
    "                    x_sen_train = X_sen_train.loc[:, sensitive_feature]\n",
    "                    cov = Cov(x_sen_train, y_pred.loc[:,'y'])\n",
    "                    print('Group:', sensitive_feature)\n",
    "                    print('  cov:', cov)\n",
    "        \n",
    "                print('\\nCurrent performance information:')\n",
    "                tpr,_,fpr,_,_,acc,_ = self.score(X_train,\n",
    "                                                 y_train,\n",
    "                                                 keys_list=list(start_thresholds.keys()),\n",
    "                                                 thresholds_array=thresholds_array)\n",
    "                _,_,di = self.score_fairness(X_train,\n",
    "                                             X_sen_train,\n",
    "                                             y_train,\n",
    "                                             keys_list=list(start_thresholds.keys()),\n",
    "                                             thresholds_array=thresholds_array)\n",
    "                print('TPR, FPR, ACC, DI:', tpr, fpr, acc, di)\n",
    "                \n",
    "            return np.array(F)\n",
    "        \n",
    "        # -- run minimization of objective\n",
    "        start_thresholds_array = list(start_thresholds.values())\n",
    "        start = time.time()\n",
    "        result = minimize(fun=F, \n",
    "                          x0=start_thresholds_array,  \n",
    "                          method=\"Nelder-Mead\")\n",
    "        end = time.time()\n",
    "        print('\\n{} (Time: {} sec.)'.format(result.message, end - start))\n",
    "        thresholds_array = result.x\n",
    "        self.thresholds = dict(zip(start_thresholds.keys(), thresholds_array))\n",
    "    \n",
    "        # -- store trained classification model (leak detector) per sensor node\n",
    "        self.fit_model(self.thresholds, print_coeff=print_coeff)\n",
    "        \n",
    "class ETC_optimizeFTPR_F_ndb(EnsembleThresholdClassification):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 alias='T-F-PR+F-ndb',\n",
    "                 classifier=ThresholdClassification, \n",
    "                 classifier_approx=ThresholdClassificationApproximation):\n",
    "        super().__init__(classifier, classifier_approx)\n",
    "        self.alias = alias\n",
    "            \n",
    "    def fit(self, \n",
    "            X_train,\n",
    "            X_sen_train,\n",
    "            y_train,\n",
    "            start_thresholds,\n",
    "            c=0.5,\n",
    "            mu=0.1,\n",
    "            barrier='log',\n",
    "            print_coeff=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Train residual based threshold classification model (leak detector) \n",
    "        per sensor node\n",
    "        based on the (pressure) residual inputs per sensor nodes\n",
    "        by learning one threshold per sensor node\n",
    "        by minimizing the exact objective F = -TPR + FPR \n",
    "        over these thresholds\n",
    "        under some exact covariance based side constraints,\n",
    "        put into a log barrier or max barrier penalty.\n",
    "        \n",
    "        Input:\n",
    "        X_train:            dataframe of training (pressure) residual inputs \n",
    "                            of shape samples x sensors\n",
    "        X_sen_train         dataframe of test sensitive (feature) inputs \n",
    "                            of shape samples x sensitive features\n",
    "        y_train:            dataframe of training (leak y/n) labels \n",
    "                            of shape samples x 1\n",
    "        start_thresholds:   numpy array of thresholds used for each sensor node\n",
    "                            of shape nodes x 1\n",
    "        c:                  positive float that indicates\n",
    "                            the upper bound of the absolute value\n",
    "                            of the side constraint(s)\n",
    "        mu:                 positive float that indicates\n",
    "                            the hyperparameter of the log or max barrier method  \n",
    "        barrier:            string of either 'log' or 'max' to choose \n",
    "                            the barrier method\n",
    "        print_coeff:        boolean which indicates whether trained coefficients\n",
    "                            should be printed or not\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- train one classification model (leak detector) per sensor node\n",
    "        # --- by minimizing an objective of the ensemble classification model\n",
    "        \n",
    "        # -- define objective\n",
    "        def F(thresholds_array):\n",
    "            _, y_pred = self.predict(X_train, \n",
    "                                     keys_list=list(start_thresholds.keys()),\n",
    "                                     thresholds_array=thresholds_array)\n",
    "            F = -TPR(y_train,y_pred) + FPR(y_train,y_pred)\n",
    "            \n",
    "            if print_coeff:\n",
    "                print('\\n\\nCurrent thresholds:')\n",
    "                for threshold in thresholds_array:\n",
    "                    print(threshold)\n",
    "                \n",
    "                print('\\nCurrent objective and fairness information:')\n",
    "                print('TPR:', TPR(y_train,y_pred))\n",
    "                print('FPR:', FPR(y_train,y_pred))\n",
    "                print('F:  ', F)\n",
    "            \n",
    "            sensitive_features = list(X_sen_train.columns)\n",
    "            for sensitive_feature in sensitive_features:\n",
    "                x_sen_train = X_sen_train.loc[:, sensitive_feature]\n",
    "                cov = Cov(x_sen_train, y_pred.loc[:,'y'])\n",
    "                if barrier == 'log':\n",
    "                    F -= mu * log_on_R(c - cov)\n",
    "                    F -= mu * log_on_R(c + cov)\n",
    "                if barrier == 'max':\n",
    "                    F += mu * max(0, -(c - cov))\n",
    "                    F += mu * max(0, -(c + cov))\n",
    "\n",
    "                if print_coeff:\n",
    "                    print('Group:', sensitive_feature)\n",
    "                    print('  cov:', cov)\n",
    "                    if barrier == 'log':\n",
    "                        print('  - mu * log_on_R(c - cov):', - mu * log_on_R(c - cov))\n",
    "                        print('  - mu * log_on_R(c + cov):', - mu * log_on_R(c + cov))\n",
    "                    if barrier == 'max':\n",
    "                        print('  + mu * max(0, -(c - cov)):', mu * max(0, -(c - cov)))\n",
    "                        print('  + mu * max(0, -(c + cov)):', mu * max(0, -(c + cov)))\n",
    "                    print('  F:', F)\n",
    "                    \n",
    "            if print_coeff:\n",
    "                print('\\nCurrent performance information:')\n",
    "                tpr,_,fpr,_,_,acc,_ = self.score(X_train,\n",
    "                                                 y_train,\n",
    "                                                 keys_list=list(start_thresholds.keys()),\n",
    "                                                 thresholds_array=thresholds_array)\n",
    "                _,_,di = self.score_fairness(X_train,\n",
    "                                             X_sen_train,\n",
    "                                             y_train,\n",
    "                                             keys_list=list(start_thresholds.keys()),\n",
    "                                             thresholds_array=thresholds_array)\n",
    "                print('TPR, FPR, ACC, DI:', tpr, fpr, acc, di)\n",
    "            \n",
    "            return np.array(F)\n",
    "        \n",
    "        # -- run minimization of objective\n",
    "        start_thresholds_array = list(start_thresholds.values())\n",
    "        start = time.time()\n",
    "        result = minimize(fun=F, \n",
    "                          x0=start_thresholds_array,  \n",
    "                          method=\"Nelder-Mead\")\n",
    "        end = time.time()\n",
    "        print('\\n{} (Time: {} sec.)'.format(result.message, end - start))\n",
    "        thresholds_array = result.x\n",
    "        self.thresholds = dict(zip(start_thresholds.keys(), thresholds_array))\n",
    "    \n",
    "        # -- store trained classification model (leak detector) per sensor node\n",
    "        self.fit_model(self.thresholds, print_coeff=print_coeff)\n",
    "        \n",
    "class ETC_optimizeACC_db(EnsembleThresholdClassification):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 alias='ACC',\n",
    "                 classifier=ThresholdClassification, \n",
    "                 classifier_approx=ThresholdClassificationApproximation):\n",
    "        super().__init__(classifier, classifier_approx)\n",
    "        self.alias = alias\n",
    "            \n",
    "    def fit(self, \n",
    "            X_train, \n",
    "            X_sen_train,\n",
    "            y_train,\n",
    "            start_thresholds,\n",
    "            b_sigmoid=100, \n",
    "            sum_threshold=0.8,\n",
    "            print_coeff=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Train residual based threshold classification model (leak detector) \n",
    "        per sensor node\n",
    "        based on the (pressure) residual inputs per sensor nodes\n",
    "        by learning one threshold per sensor node\n",
    "        by minimizing the db. approximation of the objective F = -AUC\n",
    "        over these thresholds.\n",
    "        \n",
    "        Input:\n",
    "        X_train:            dataframe of training (pressure) residual inputs \n",
    "                            of shape samples x sensors\n",
    "        X_sen_train         dataframe of training sensitive (feature) inputs \n",
    "                            of shape samples x sensitive features\n",
    "                            (only required for evaluation if print_coeff=True)\n",
    "        y_train:            dataframe of training (leak y/n) labels \n",
    "                            of shape samples x 1\n",
    "        start_thresholds:   numpy array of thresholds used for each sensor node\n",
    "                            of shape nodes x 1\n",
    "        b_sigmoid:          positive float that indicates \n",
    "                            the b in the sigmoid function\n",
    "        sum_thresholds:     positive float that indicates\n",
    "                            when the sum of sensor node outputs indicates a leak\n",
    "        print_coeff:        boolean which indicates whether trained coefficients\n",
    "                            should be printed or not\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- train one classification model (leak detector) per sensor node\n",
    "        # --- by minimizing an objective of the ensemble classification model\n",
    "        \n",
    "        # -- define objective and objective's gradient\n",
    "        def F(thresholds_array):\n",
    "            _, y_pred = self.predict_approx(X_train, \n",
    "                                            keys_list=list(start_thresholds.keys()),\n",
    "                                            thresholds_array=thresholds_array, \n",
    "                                            b_sigmoid=b_sigmoid,\n",
    "                                            sum_threshold=sum_threshold)\n",
    "            F = -ACC(y_train,y_pred)\n",
    "            \n",
    "            if print_coeff:\n",
    "                print('\\n\\nCurrent thresholds:')\n",
    "                for threshold in thresholds_array:\n",
    "                    print(threshold)\n",
    "                \n",
    "                print('\\nCurrent objective information:')\n",
    "                print('ACC:', ACC(y_train,y_pred))\n",
    "                print('F:  ', F)\n",
    "                \n",
    "                print('\\nCurrent fairness information:')\n",
    "                sensitive_features = list(X_sen_train.columns)\n",
    "                for sensitive_feature in sensitive_features:\n",
    "                    x_sen_train = X_sen_train.loc[:, sensitive_feature]\n",
    "                    cov = Cov(x_sen_train, y_pred.loc[:,'y'])\n",
    "                    print('Group:', sensitive_feature)\n",
    "                    print('  cov:', cov)\n",
    "        \n",
    "                print('\\nCurrent performance information:')\n",
    "                tpr,_,fpr,_,_,acc,_ = self.score(X_train,\n",
    "                                                 y_train,\n",
    "                                                 keys_list=list(start_thresholds.keys()),\n",
    "                                                 thresholds_array=thresholds_array)\n",
    "                _,_,di = self.score_fairness(X_train,\n",
    "                                             X_sen_train,\n",
    "                                             y_train,\n",
    "                                             keys_list=list(start_thresholds.keys()),\n",
    "                                             thresholds_array=thresholds_array)\n",
    "                print('TPR, FPR, ACC, DI:', tpr, fpr, acc, di)\n",
    "                \n",
    "            return np.array(F)\n",
    "            \n",
    "        def gradF(thresholds_array):\n",
    "            dx_Y_pred = self.dx_predict_approx(X_train,\n",
    "                                               keys_list=list(start_thresholds.keys()),\n",
    "                                               thresholds_array=thresholds_array, \n",
    "                                               b_sigmoid=b_sigmoid,\n",
    "                                               sum_threshold=sum_threshold)\n",
    "            gradF = list()\n",
    "            sensor_ids = list(X_train.columns)\n",
    "            for node in sensor_ids:\n",
    "                dx_y_pred = dx_Y_pred.loc[:,node]\n",
    "                dx_y_pred = pd.DataFrame(data=dx_y_pred,\n",
    "                                         index=dx_y_pred.index)\n",
    "                dx_y_pred = dx_y_pred.rename({node:'y'},\n",
    "                                              axis='columns')\n",
    "                partialF = -dx_ACC(y_train,dx_y_pred)\n",
    "                gradF.append(partialF)\n",
    "                \n",
    "            if print_coeff:\n",
    "                print('\\ngradF:', gradF)  \n",
    "                \n",
    "            return np.array(gradF)\n",
    "        \n",
    "        # -- run minimization of objective\n",
    "        start_thresholds_array = list(start_thresholds.values())\n",
    "        start = time.time()\n",
    "        result = minimize(fun=F, \n",
    "                          x0=start_thresholds_array, \n",
    "                          jac=gradF, \n",
    "                          method=\"BFGS\")\n",
    "        end = time.time()\n",
    "        print('\\n{} (Time: {} sec.)'.format(result.message, end - start))\n",
    "        thresholds_array = result.x\n",
    "        self.thresholds = dict(zip(start_thresholds.keys(), thresholds_array))\n",
    "    \n",
    "        # -- store trained classification model (leak detector) per sensor node\n",
    "        self.fit_model(self.thresholds, print_coeff=print_coeff)\n",
    "            \n",
    "            \n",
    "class ETC_optimizeACC_F_db(EnsembleThresholdClassification):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 alias='ACC+F',\n",
    "                 classifier=ThresholdClassification, \n",
    "                 classifier_approx=ThresholdClassificationApproximation):\n",
    "        super().__init__(classifier, classifier_approx)\n",
    "        self.alias = alias\n",
    "            \n",
    "    def fit(self, \n",
    "            X_train,\n",
    "            X_sen_train,\n",
    "            y_train,\n",
    "            start_thresholds,\n",
    "            b_sigmoid=100, \n",
    "            sum_threshold=0.8,\n",
    "            c=0.5,\n",
    "            mu=0.1,\n",
    "            print_coeff=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Train residual based threshold classification model (leak detector) \n",
    "        per sensor node\n",
    "        based on the (pressure) residual inputs per sensor nodes\n",
    "        by learning one threshold per sensor node\n",
    "        by minimizing the db. approximation of the objective F = -AUC\n",
    "        over these thresholds\n",
    "        under some db. approximation covariance based side constraints, \n",
    "        put into a log barrier penalty.\n",
    "        \n",
    "        Input:\n",
    "        X_train:            dataframe of training (pressure) residual inputs \n",
    "                            of shape samples x sensors\n",
    "        X_sen_train         dataframe of training sensitive (feature) inputs \n",
    "                            of shape samples x sensitive features\n",
    "        y_train:            dataframe of training (leak y/n) labels \n",
    "                            of shape samples x 1\n",
    "        start_thresholds:   numpy array of thresholds used for each sensor node\n",
    "                            of shape nodes x 1\n",
    "        b_sigmoid:          positive float that indicates \n",
    "                            the b in the sigmoid function\n",
    "        sum_thresholds:     positive float that indicates\n",
    "                            when the sum of sensor node outputs indicates a leak\n",
    "        c:                  positive float that indicates\n",
    "                            the upper bound of the absolute value\n",
    "                            of the side constraint(s)\n",
    "        mu:                 positive float that indicates\n",
    "                            the hyperparameter of the log barrier method                    \n",
    "        print_coeff:        boolean which indicates whether trained coefficients\n",
    "                            should be printed or not\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- train one classification model (leak detector) per sensor node\n",
    "        # --- by minimizing an objective of the ensemble classification model\n",
    "        \n",
    "        # -- define objective and objective's gradient\n",
    "        def F(thresholds_array):\n",
    "            _, y_pred = self.predict_approx(X_train, \n",
    "                                            keys_list=list(start_thresholds.keys()),\n",
    "                                            thresholds_array=thresholds_array, \n",
    "                                            b_sigmoid=b_sigmoid,\n",
    "                                            sum_threshold=sum_threshold)\n",
    "            F = -ACC(y_train,y_pred)\n",
    "            \n",
    "            if print_coeff:\n",
    "                print('\\n\\nCurrent thresholds:')\n",
    "                for threshold in thresholds_array:\n",
    "                    print(threshold)\n",
    "                \n",
    "                print('\\nCurrent objective and fairness information:')\n",
    "                print('ACC:', ACC(y_train,y_pred))\n",
    "                print('F:  ', F)\n",
    "            \n",
    "            sensitive_features = list(X_sen_train.columns)\n",
    "            for sensitive_feature in sensitive_features:\n",
    "                x_sen_train = X_sen_train.loc[:, sensitive_feature]\n",
    "                cov = Cov(x_sen_train, y_pred.loc[:,'y'])\n",
    "                F -= mu * log_on_R(c - cov)\n",
    "                F -= mu * log_on_R(c + cov)\n",
    "                \n",
    "                if print_coeff:\n",
    "                    print('Group:', sensitive_feature)\n",
    "                    print('  cov:', cov)\n",
    "                    print('  - mu * log_on_R(c - cov):', - mu * log_on_R(c - cov))\n",
    "                    print('  - mu * log_on_R(c + cov):', - mu * log_on_R(c + cov))\n",
    "                    print('  F:', F)\n",
    "                    \n",
    "            if print_coeff:\n",
    "                print('\\nCurrent performance information:')\n",
    "                tpr,_,fpr,_,_,acc,_ = self.score(X_train,\n",
    "                                                 y_train,\n",
    "                                                 keys_list=list(start_thresholds.keys()),\n",
    "                                                 thresholds_array=thresholds_array)\n",
    "                _,_,di = self.score_fairness(X_train,\n",
    "                                             X_sen_train,\n",
    "                                             y_train,\n",
    "                                             keys_list=list(start_thresholds.keys()),\n",
    "                                             thresholds_array=thresholds_array)\n",
    "                print('TPR, FPR, ACC, DI:', tpr, fpr, acc, di)\n",
    "            \n",
    "            return np.array(F)\n",
    "            \n",
    "        def gradF(thresholds_array):\n",
    "            _, y_pred = self.predict_approx(X_train, \n",
    "                                            keys_list=list(start_thresholds.keys()),\n",
    "                                            thresholds_array=thresholds_array, \n",
    "                                            b_sigmoid=b_sigmoid,\n",
    "                                            sum_threshold=sum_threshold)\n",
    "            dx_Y_pred = self.dx_predict_approx(X_train,\n",
    "                                               keys_list=list(start_thresholds.keys()),\n",
    "                                               thresholds_array=thresholds_array, \n",
    "                                               b_sigmoid=b_sigmoid,\n",
    "                                               sum_threshold=sum_threshold)\n",
    "            gradF = list()\n",
    "            sensor_ids = list(X_train.columns)\n",
    "            for node in sensor_ids:\n",
    "                dx_y_pred = dx_Y_pred.loc[:,node]\n",
    "                dx_y_pred = pd.DataFrame(data=dx_y_pred,\n",
    "                                         index=dx_y_pred.index)\n",
    "                dx_y_pred = dx_y_pred.rename({node:'y'},\n",
    "                                              axis='columns')\n",
    "                partialF = -dx_ACC(y_train,dx_y_pred)\n",
    "                \n",
    "                sensitive_features = list(X_sen_train.columns)\n",
    "                for sensitive_feature in sensitive_features:\n",
    "                    x_sen_train = X_sen_train.loc[:, sensitive_feature]\n",
    "                    cov = Cov(x_sen_train, y_pred.loc[:,'y'])\n",
    "                    dx_cov = Cov(x_sen_train, dx_y_pred.loc[:,'y'])\n",
    "                    partialF -= (mu * -1 * dx_cov) / (c - cov)\n",
    "                    partialF -= (mu * dx_cov) / (c + cov)\n",
    "                gradF.append(partialF)\n",
    "                \n",
    "            if print_coeff:    \n",
    "                print('\\ngradF:', gradF) \n",
    "                \n",
    "            return np.array(gradF)\n",
    "        \n",
    "        # -- run minimization of objective\n",
    "        start_thresholds_array = list(start_thresholds.values())\n",
    "        start = time.time()\n",
    "        result = minimize(fun=F, \n",
    "                          x0=start_thresholds_array, \n",
    "                          jac=gradF, \n",
    "                          method=\"BFGS\")\n",
    "        end = time.time()\n",
    "        print('\\n{} (Time: {} sec.)'.format(result.message, end - start))\n",
    "        thresholds_array = result.x\n",
    "        self.thresholds = dict(zip(start_thresholds.keys(), thresholds_array))\n",
    "    \n",
    "        # -- store trained classification model (leak detector) per sensor node\n",
    "        self.fit_model(self.thresholds, print_coeff=print_coeff)\n",
    "        \n",
    "class ETC_optimizeACC_ndb(EnsembleThresholdClassification):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 alias='ACC-ndb',\n",
    "                 classifier=ThresholdClassification, \n",
    "                 classifier_approx=ThresholdClassificationApproximation):\n",
    "        super().__init__(classifier, classifier_approx)\n",
    "        self.alias = alias\n",
    "            \n",
    "    def fit(self, \n",
    "            X_train, \n",
    "            X_sen_train,\n",
    "            y_train,\n",
    "            start_thresholds,\n",
    "            print_coeff=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Train residual based threshold classification model (leak detector) \n",
    "        per sensor node\n",
    "        based on the (pressure) residual inputs per sensor nodes\n",
    "        by learning one threshold per sensor node\n",
    "        by minimizing the exact objective F = -AUC\n",
    "        over these thresholds.\n",
    "        \n",
    "        Input:\n",
    "        X_train:            dataframe of training (pressure) residual inputs \n",
    "                            of shape samples x sensors\n",
    "        X_sen_train         dataframe of training sensitive (feature) inputs \n",
    "                            of shape samples x sensitive features\n",
    "                            (only required for evaluation if print_coeff=True)\n",
    "        y_train:            dataframe of training (leak y/n) labels \n",
    "                            of shape samples x 1\n",
    "        start_thresholds:   numpy array of thresholds used for each sensor node\n",
    "                            of shape nodes x 1\n",
    "        print_coeff:        boolean which indicates whether trained coefficients\n",
    "                            should be printed or not\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- train one classification model (leak detector) per sensor node\n",
    "        # --- by minimizing an objective of the ensemble classification model\n",
    "        \n",
    "        # -- define objective\n",
    "        def F(thresholds_array):\n",
    "            _, y_pred = self.predict(X_train, \n",
    "                                     keys_list=list(start_thresholds.keys()),\n",
    "                                     thresholds_array=thresholds_array)\n",
    "            F = -ACC(y_train,y_pred)\n",
    "            \n",
    "            if print_coeff:\n",
    "                print('\\n\\nCurrent thresholds:')\n",
    "                for threshold in thresholds_array:\n",
    "                    print(threshold)\n",
    "                \n",
    "                print('\\nCurrent objective information:')\n",
    "                print('ACC:', ACC(y_train,y_pred))\n",
    "                print('F:  ', F)\n",
    "                \n",
    "                print('\\nCurrent fairness information:')\n",
    "                sensitive_features = list(X_sen_train.columns)\n",
    "                for sensitive_feature in sensitive_features:\n",
    "                    x_sen_train = X_sen_train.loc[:, sensitive_feature]\n",
    "                    cov = Cov(x_sen_train, y_pred.loc[:,'y'])\n",
    "                    print('Group:', sensitive_feature)\n",
    "                    print('  cov:', cov)\n",
    "        \n",
    "                print('\\nCurrent performance information:')\n",
    "                tpr,_,fpr,_,_,acc,_ = self.score(X_train,\n",
    "                                                 y_train,\n",
    "                                                 keys_list=list(start_thresholds.keys()),\n",
    "                                                 thresholds_array=thresholds_array)\n",
    "                _,_,di = self.score_fairness(X_train,\n",
    "                                             X_sen_train,\n",
    "                                             y_train,\n",
    "                                             keys_list=list(start_thresholds.keys()),\n",
    "                                             thresholds_array=thresholds_array)\n",
    "                print('TPR, FPR, ACC, DI:', tpr, fpr, acc, di)\n",
    "            \n",
    "            return np.array(F)\n",
    "        \n",
    "        # -- run minimization of objective\n",
    "        start_thresholds_array = list(start_thresholds.values())\n",
    "        start = time.time()\n",
    "        result = minimize(fun=F, \n",
    "                          x0=start_thresholds_array,  \n",
    "                          method=\"Nelder-Mead\")\n",
    "        end = time.time()\n",
    "        print('\\n{} (Time: {} sec.)'.format(result.message, end - start))\n",
    "        thresholds_array = result.x\n",
    "        self.thresholds = dict(zip(start_thresholds.keys(), thresholds_array))\n",
    "    \n",
    "        # -- store trained classification model (leak detector) per sensor node\n",
    "        self.fit_model(self.thresholds, print_coeff=print_coeff)\n",
    "        \n",
    "class ETC_optimizeACC_F_ndb(EnsembleThresholdClassification):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 alias='ACC+F-ndb',\n",
    "                 classifier=ThresholdClassification, \n",
    "                 classifier_approx=ThresholdClassificationApproximation):\n",
    "        super().__init__(classifier, classifier_approx)\n",
    "        self.alias = alias\n",
    "            \n",
    "    def fit(self, \n",
    "            X_train,\n",
    "            X_sen_train,\n",
    "            y_train,\n",
    "            start_thresholds,\n",
    "            c=0.5,\n",
    "            mu=0.1,\n",
    "            barrier='log',\n",
    "            print_coeff=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Train residual based threshold classification model (leak detector) \n",
    "        per sensor node\n",
    "        based on the (pressure) residual inputs per sensor nodes\n",
    "        by learning one threshold per sensor node\n",
    "        by minimizing the exact objective F = -AUC\n",
    "        over these thresholds\n",
    "        under some exact covariance based side constraints,\n",
    "        put into a log barrier or max barrier penalty.\n",
    "        \n",
    "        Input:\n",
    "        X_train:            dataframe of training (pressure) residual inputs \n",
    "                            of shape samples x sensors\n",
    "        X_sen_train         dataframe of training sensitive (feature) inputs \n",
    "                            of shape samples x sensitive features\n",
    "        y_train:            dataframe of training (leak y/n) labels \n",
    "                            of shape samples x 1\n",
    "        start_thresholds:   numpy array of thresholds used for each sensor node\n",
    "                            of shape nodes x 1\n",
    "        c:                  positive float that indicates\n",
    "                            the upper bound of the absolute value\n",
    "                            of the side constraint(s)\n",
    "        mu:                 positive float that indicates\n",
    "                            the hyperparameter of the log or max barrier method\n",
    "        barrier:            string of either 'log' or 'max' to choose \n",
    "                            the barrier method\n",
    "        print_coeff:        boolean which indicates whether trained coefficients\n",
    "                            should be printed or not\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- train one classification model (leak detector) per sensor node\n",
    "        # --- by minimizing an objective of the ensemble classification model\n",
    "        \n",
    "        # -- define objective\n",
    "        def F(thresholds_array):\n",
    "            _, y_pred = self.predict(X_train, \n",
    "                                     keys_list=list(start_thresholds.keys()),\n",
    "                                     thresholds_array=thresholds_array)\n",
    "            F = -ACC(y_train,y_pred)\n",
    "            \n",
    "            if print_coeff:\n",
    "                print('\\n\\nCurrent thresholds:')\n",
    "                for threshold in thresholds_array:\n",
    "                    print(threshold)\n",
    "                \n",
    "                print('\\nCurrent objective and fairness information:')\n",
    "                print('ACC:', ACC(y_train,y_pred))\n",
    "                print('F:  ', F)\n",
    "            \n",
    "            sensitive_features = list(X_sen_train.columns)\n",
    "            for sensitive_feature in sensitive_features:\n",
    "                x_sen_train = X_sen_train.loc[:, sensitive_feature]\n",
    "                cov = Cov(x_sen_train, y_pred.loc[:,'y'])\n",
    "                if barrier == 'log':\n",
    "                    F -= mu * log_on_R(c - cov)\n",
    "                    F -= mu * log_on_R(c + cov)\n",
    "                if barrier == 'max':\n",
    "                    F += mu * max(0, -(c - cov))\n",
    "                    F += mu * max(0, -(c + cov))\n",
    "\n",
    "                if print_coeff:\n",
    "                    print('Group:', sensitive_feature)\n",
    "                    print('  cov:', cov)\n",
    "                    if barrier == 'log':\n",
    "                        print('  - mu * log_on_R(c - cov):', - mu * log_on_R(c - cov))\n",
    "                        print('  - mu * log_on_R(c + cov):', - mu * log_on_R(c + cov))\n",
    "                    if barrier == 'max':\n",
    "                        print('  + mu * max(0, -(c - cov)):', mu * max(0, -(c - cov)))\n",
    "                        print('  + mu * max(0, -(c + cov)):', mu * max(0, -(c + cov)))\n",
    "                    print('  F:', F)\n",
    "                    \n",
    "            if print_coeff:\n",
    "                print('\\nCurrent performance information:')\n",
    "                tpr,_,fpr,_,_,acc,_ = self.score(X_train,\n",
    "                                                 y_train,\n",
    "                                                 keys_list=list(start_thresholds.keys()),\n",
    "                                                 thresholds_array=thresholds_array)\n",
    "                _,_,di = self.score_fairness(X_train,\n",
    "                                             X_sen_train,\n",
    "                                             y_train,\n",
    "                                             keys_list=list(start_thresholds.keys()),\n",
    "                                             thresholds_array=thresholds_array)\n",
    "                print('TPR, FPR, ACC, DI:', tpr, fpr, acc, di)\n",
    "            \n",
    "            return np.array(F)\n",
    "        \n",
    "        # -- run minimization of objective\n",
    "        start_thresholds_array = list(start_thresholds.values())\n",
    "        start = time.time()\n",
    "        result = minimize(fun=F, \n",
    "                          x0=start_thresholds_array,  \n",
    "                          method=\"Nelder-Mead\")\n",
    "        end = time.time()\n",
    "        print('\\n{} (Time: {} sec.)'.format(result.message, end - start))\n",
    "        thresholds_array = result.x\n",
    "        self.thresholds = dict(zip(start_thresholds.keys(), thresholds_array))\n",
    "    \n",
    "        # -- store trained classification model (leak detector) per sensor node\n",
    "        self.fit_model(self.thresholds, print_coeff=print_coeff)\n",
    "        \n",
    "class ETC_optimizeCOV_ndb(EnsembleThresholdClassification):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 alias='COV+ACC-ndb',\n",
    "                 classifier=ThresholdClassification, \n",
    "                 classifier_approx=ThresholdClassificationApproximation):\n",
    "        super().__init__(classifier, classifier_approx)\n",
    "        self.alias = alias\n",
    "        \n",
    "    def fit(self, \n",
    "            X_train,\n",
    "            X_sen_train,\n",
    "            y_train,\n",
    "            start_thresholds,\n",
    "            mu=0.01,\n",
    "            lamb=0.04,\n",
    "            barrier='log',\n",
    "            acc_best=1,\n",
    "            print_coeff=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Train residual based threshold classification model (leak detector) \n",
    "        per sensor node\n",
    "        based on the (pressure) residual inputs per sensor nodes\n",
    "        by learning one threshold per sensor node\n",
    "        by minimizing the exact objective F = -\\sum_k Cov(S_k,\\hat{Y})\n",
    "        over these thresholds\n",
    "        under some exact accuracy based side constraints,\n",
    "        put into a log barrier or max barrier penalty.\n",
    "        \n",
    "        Input:\n",
    "        X_train:            dataframe of training (pressure) residual inputs \n",
    "                            of shape samples x sensors\n",
    "        X_sen_train         dataframe of test sensitive (feature) inputs \n",
    "                            of shape samples x sensitive features\n",
    "        y_train:            dataframe of training (leak y/n) labels \n",
    "                            of shape samples x 1\n",
    "        start_thresholds:   numpy array of thresholds used for each sensor node\n",
    "                            of shape nodes x 1\n",
    "        mu:                 positive float that indicates\n",
    "                            the hyperparameter of the log or max barrier method\n",
    "        lamb:               positive float that indicates\n",
    "                            the percentage we allow to deviate from acc_best\n",
    "        barrier:            string of either 'log' or 'max' to choose \n",
    "                            the barrier method\n",
    "        acc_best:           positive float between 0 and 1 that indicates\n",
    "                            the best accuarcy reach in this optimization problem\n",
    "                            without considering fairness constraints\n",
    "        print_coeff:        boolean which indicates whether trained coefficients\n",
    "                            should be printed or not\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- train one classification model (leak detector) per sensor node\n",
    "        # --- by minimizing an objective of the ensemble classification model\n",
    "        \n",
    "        # -- define objective\n",
    "        def F(thresholds_array):\n",
    "            _, y_pred = self.predict(X_train, \n",
    "                                     keys_list=list(start_thresholds.keys()),\n",
    "                                     thresholds_array=thresholds_array)\n",
    "            F = 0\n",
    "            \n",
    "            if print_coeff:\n",
    "                print('\\n\\nCurrent thresholds:')\n",
    "                for threshold in thresholds_array:\n",
    "                    print(threshold)\n",
    "                \n",
    "                print('\\nCurrent objective and fairness information:')\n",
    "            \n",
    "            sensitive_features = list(X_sen_train.columns)\n",
    "            for sensitive_feature in sensitive_features:\n",
    "                x_sen_train = X_sen_train.loc[:, sensitive_feature]\n",
    "                cov = Cov(x_sen_train, y_pred.loc[:,'y'])\n",
    "                F += abs(cov)\n",
    "                \n",
    "                if print_coeff:\n",
    "                    print('Group:', sensitive_feature)\n",
    "                    print('  cov:', cov)\n",
    "                    print('  F:', F)\n",
    "                \n",
    "            acc = ACC(y_train,y_pred)\n",
    "            if barrier == 'log':\n",
    "                F -= mu * log_on_R(acc - (1-lamb)*acc_best)\n",
    "            if barrier == 'max':\n",
    "                F += mu * max(0, -(acc - (1-lamb)*acc_best))\n",
    "            \n",
    "            if print_coeff:\n",
    "                print('ACC:                                      ', acc)\n",
    "                print('acc - (1-lamb)*acc_best:                  ', acc - (1-lamb)*acc_best)\n",
    "                if barrier == 'log':\n",
    "                    print('- mu * log_on_R(acc - (1-lamb)*acc_best): ', - mu * log_on_R(acc - (1-lamb)*acc_best))\n",
    "                if barrier == 'max':\n",
    "                    print('+ mu * max(0, -(acc - (1-lamb)*acc_best)):', mu * max(0, -(acc - (1-lamb)*acc_best)))\n",
    "                print('F:                                        ', F)\n",
    "                \n",
    "                print('\\nCurrent performance information:')\n",
    "                tpr,_,fpr,_,_,acc,_ = self.score(X_train,\n",
    "                                                 y_train,\n",
    "                                                 keys_list=list(start_thresholds.keys()),\n",
    "                                                 thresholds_array=thresholds_array)\n",
    "                _,_,di = self.score_fairness(X_train,\n",
    "                                             X_sen_train,\n",
    "                                             y_train,\n",
    "                                             keys_list=list(start_thresholds.keys()),\n",
    "                                             thresholds_array=thresholds_array)\n",
    "                print('TPR, FPR, ACC, DI:', tpr, fpr, acc, di)\n",
    "            \n",
    "            return np.array(F)\n",
    "        \n",
    "        # -- run minimization of objective\n",
    "        start_thresholds_array = list(start_thresholds.values())\n",
    "        start = time.time()\n",
    "        result = minimize(fun=F, \n",
    "                          x0=start_thresholds_array,  \n",
    "                          method=\"Nelder-Mead\")\n",
    "        end = time.time()\n",
    "        print('\\n{} (Time: {} sec.)'.format(result.message, end - start))\n",
    "        thresholds_array = result.x\n",
    "        self.thresholds = dict(zip(start_thresholds.keys(), thresholds_array))\n",
    "    \n",
    "        # -- store trained classification model (leak detector) per sensor node\n",
    "        self.fit_model(self.thresholds, print_coeff=print_coeff)    \n",
    "        \n",
    "class ETC_optimizeEO_ndb(EnsembleThresholdClassification):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 alias='EO+ACC-ndb',\n",
    "                 classifier=ThresholdClassification, \n",
    "                 classifier_approx=ThresholdClassificationApproximation):\n",
    "        super().__init__(classifier, classifier_approx)\n",
    "        self.alias = alias\n",
    "        \n",
    "    def fit(self, \n",
    "            X_train,\n",
    "            X_sen_train,\n",
    "            y_train,\n",
    "            start_thresholds,\n",
    "            mu=0.1,\n",
    "            lamb=0.1,\n",
    "            barrier='log',\n",
    "            acc_best=1,\n",
    "            print_coeff=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Train residual based threshold classification model (leak detector) \n",
    "        per sensor node\n",
    "        based on the (pressure) residual inputs per sensor nodes\n",
    "        by learning one threshold per sensor node\n",
    "        by minimizing the exact objective F = EO\n",
    "        over these thresholds\n",
    "        under some exact accuracy based side constraints,\n",
    "        put into a log barrier or max barrier penalty.\n",
    "        \n",
    "        Input:\n",
    "        X_train:            dataframe of training (pressure) residual inputs \n",
    "                            of shape samples x sensors\n",
    "        X_sen_train         dataframe of test sensitive (feature) inputs \n",
    "                            of shape samples x sensitive features\n",
    "        y_train:            dataframe of training (leak y/n) labels \n",
    "                            of shape samples x 1\n",
    "        start_thresholds:   numpy array of thresholds used for each sensor node\n",
    "                            of shape nodes x 1\n",
    "        mu:                 positive float that indicates\n",
    "                            the hyperparameter of the log or max barrier method\n",
    "        lamb:               positive float that indicates\n",
    "                            the percentage we allow to deviate from acc_best\n",
    "        barrier:            string of either 'log' or 'max' to choose \n",
    "                            the barrier method\n",
    "        acc_best:           positive float between 0 and 1 that indicates\n",
    "                            the best accuarcy reach in this optimization problem\n",
    "                            without considering fairness constraints\n",
    "        print_coeff:        boolean which indicates whether trained coefficients\n",
    "                            should be printed or not\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- train one classification model (leak detector) per sensor node\n",
    "        # --- by minimizing an objective of the ensemble classification model\n",
    "        \n",
    "        # -- define objective \n",
    "        def F(thresholds_array):\n",
    "            _, y_pred = self.predict(X_train, \n",
    "                                     keys_list=\n",
    "                                     list(start_thresholds.keys()),\n",
    "                                     thresholds_array=thresholds_array)\n",
    "            _,eo,_ = self.score_fairness(X_train,\n",
    "                                         X_sen_train,\n",
    "                                         y_train,\n",
    "                                         keys_list=list(start_thresholds.keys()),\n",
    "                                         thresholds_array=thresholds_array)\n",
    "            acc = ACC(y_train,y_pred)\n",
    "            if barrier == 'log':\n",
    "                F = eo - mu * log_on_R(acc - (1-lamb)*acc_best)\n",
    "            if barrier == 'max':\n",
    "                F = eo + mu * max(0, -(acc - (1-lamb)*acc_best))\n",
    "            \n",
    "            if print_coeff:\n",
    "                print('\\n\\nCurrent thresholds:')\n",
    "                for threshold in thresholds_array:\n",
    "                    print(threshold)\n",
    "                \n",
    "                print('\\nCurrent objective and fairness information:')\n",
    "                print('EO:                                       ', eo)\n",
    "                print('ACC:                                      ', acc)\n",
    "                print('acc - (1-lamb)*acc_best:                  ', acc - (1-lamb)*acc_best)\n",
    "                if barrier == 'log':\n",
    "                    print('- mu * log_on_R(acc - (1-lamb)*acc_best): ', - mu * log_on_R(acc - (1-lamb)*acc_best))\n",
    "                if barrier == 'max':\n",
    "                    print('+ mu * max(0, -(acc - (1-lamb)*acc_best)):', mu * max(0, -(acc - (1-lamb)*acc_best)))\n",
    "                print('F:                                        ', F)\n",
    "                \n",
    "                print('\\nCurrent performance information:')\n",
    "                tpr,_,fpr,_,_,acc,_ = self.score(X_train,\n",
    "                                                 y_train,\n",
    "                                                 keys_list=list(start_thresholds.keys()),\n",
    "                                                 thresholds_array=thresholds_array)\n",
    "                _,_,di = self.score_fairness(X_train,\n",
    "                                             X_sen_train,\n",
    "                                             y_train,\n",
    "                                             keys_list=list(start_thresholds.keys()),\n",
    "                                             thresholds_array=thresholds_array)\n",
    "                print('TPR, FPR, ACC, DI:', tpr, fpr, acc, di)\n",
    "                \n",
    "            return np.array(F)\n",
    "        \n",
    "        # -- run minimization of objective\n",
    "        start_thresholds_array = list(start_thresholds.values())\n",
    "        start = time.time()\n",
    "        result = minimize(fun=F, \n",
    "                          x0=start_thresholds_array,  \n",
    "                          method=\"Nelder-Mead\")\n",
    "        end = time.time()\n",
    "        print('\\n{} (Time: {} sec.)'.format(result.message, end - start))\n",
    "        thresholds_array = result.x\n",
    "        self.thresholds = dict(zip(start_thresholds.keys(), thresholds_array))\n",
    "    \n",
    "        # -- store trained classification model (leak detector) per sensor node\n",
    "        self.fit_model(self.thresholds, print_coeff=print_coeff)\n",
    "        \n",
    "class ETC_optimizeDI_ndb(EnsembleThresholdClassification):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 alias='DI+ACC',\n",
    "                 classifier=ThresholdClassification, \n",
    "                 classifier_approx=ThresholdClassificationApproximation):\n",
    "        super().__init__(classifier, classifier_approx)\n",
    "        self.alias = alias\n",
    "        \n",
    "    def fit(self, \n",
    "            X_train,\n",
    "            X_sen_train,\n",
    "            y_train,\n",
    "            start_thresholds,\n",
    "            mu=0.05,\n",
    "            lamb=0.04,\n",
    "            barrier='log',\n",
    "            acc_best=1,\n",
    "            print_coeff=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Train residual based threshold classification model (leak detector) \n",
    "        per sensor node\n",
    "        based on the (pressure) residual inputs per sensor nodes\n",
    "        by learning one threshold per sensor node\n",
    "        by minimizing the exact objective F = -DI\n",
    "        over these thresholds\n",
    "        under some exact accuracy based side constraints,\n",
    "        put into a log barrier or max barrier penalty.\n",
    "        \n",
    "        Input:\n",
    "        X_train:            dataframe of training (pressure) residual inputs \n",
    "                            of shape samples x sensors\n",
    "        X_sen_train         dataframe of test sensitive (feature) inputs \n",
    "                            of shape samples x sensitive features\n",
    "        y_train:            dataframe of training (leak y/n) labels \n",
    "                            of shape samples x 1\n",
    "        start_thresholds:   numpy array of thresholds used for each sensor node\n",
    "                            of shape nodes x 1\n",
    "        mu:                 positive float that indicates\n",
    "                            the hyperparameter of the log or max barrier method\n",
    "        lamb:               positive float that indicates\n",
    "                            the percentage we allow to deviate from acc_best\n",
    "        barrier:            string of either 'log' or 'max' to choose \n",
    "                            the barrier method\n",
    "        acc_best:           positive float between 0 and 1 that indicates\n",
    "                            the best accuarcy reach in this optimization problem\n",
    "                            without considering fairness constraints\n",
    "        print_coeff:        boolean which indicates whether trained coefficients\n",
    "                            should be printed or not\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- train one classification model (leak detector) per sensor node\n",
    "        # --- by minimizing an objective of the ensemble classification model\n",
    "        \n",
    "        # -- define objective\n",
    "        def F(thresholds_array):\n",
    "            _, y_pred = self.predict(X_train, \n",
    "                                     keys_list=\n",
    "                                     list(start_thresholds.keys()),\n",
    "                                     thresholds_array=thresholds_array)\n",
    "            _,_,di = self.score_fairness(X_train,\n",
    "                                         X_sen_train,\n",
    "                                         y_train,\n",
    "                                         keys_list=list(start_thresholds.keys()),\n",
    "                                         thresholds_array=thresholds_array)\n",
    "            acc = ACC(y_train,y_pred)\n",
    "            if barrier == 'log':\n",
    "                F = - di - mu * log_on_R(acc - (1-lamb)*acc_best)\n",
    "            if barrier == 'max':\n",
    "                F = - di + mu * max(0, -(acc - (1-lamb)*acc_best))\n",
    "            \n",
    "            if print_coeff:\n",
    "                print('\\n\\nCurrent thresholds:')\n",
    "                for threshold in thresholds_array:\n",
    "                    print(threshold)\n",
    "                \n",
    "                print('\\nCurrent objective and fairness information:')\n",
    "                print('- DI:                                     ', - di)\n",
    "                print('ACC:                                      ', acc)\n",
    "                print('acc - (1-lamb)*acc_best:                  ', acc - (1-lamb)*acc_best)\n",
    "                if barrier == 'log':\n",
    "                    print('- mu * log_on_R(acc - (1-lamb)*acc_best): ', - mu * log_on_R(acc - (1-lamb)*acc_best))            \n",
    "                if barrier == 'max':\n",
    "                    print('+ mu * max(0, -(acc - (1-lamb)*acc_best)):', mu * max(0, -(acc - (1-lamb)*acc_best)))\n",
    "                print('F:                                        ', F)\n",
    "                \n",
    "                print('\\nCurrent performance information:')\n",
    "                tpr,_,fpr,_,_,acc,_ = self.score(X_train,\n",
    "                                                 y_train,\n",
    "                                                 keys_list=list(start_thresholds.keys()),\n",
    "                                                 thresholds_array=thresholds_array)\n",
    "                _,_,di = self.score_fairness(X_train,\n",
    "                                             X_sen_train,\n",
    "                                             y_train,\n",
    "                                             keys_list=list(start_thresholds.keys()),\n",
    "                                             thresholds_array=thresholds_array)\n",
    "                print('TPR, FPR, ACC, DI:', tpr, fpr, acc, di)\n",
    "            \n",
    "            return np.array(F)\n",
    "        \n",
    "        # -- run minimization of objective\n",
    "        start_thresholds_array = list(start_thresholds.values())\n",
    "        start = time.time()\n",
    "        result = minimize(fun=F, \n",
    "                          x0=start_thresholds_array,  \n",
    "                          method=\"Nelder-Mead\")\n",
    "        end = time.time()\n",
    "        print('\\n{} (Time: {} sec.)'.format(result.message, end - start))\n",
    "        thresholds_array = result.x\n",
    "        self.thresholds = dict(zip(start_thresholds.keys(), thresholds_array))\n",
    "    \n",
    "        # -- store trained classification model (leak detector) per sensor node\n",
    "        self.fit_model(self.thresholds, print_coeff=print_coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a488b2a-e6cc-47af-a67f-b7b4c4715979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_differences(X_train,\n",
    "                        y_train,\n",
    "                        thresholds):\n",
    "    \n",
    "    \"\"\"\n",
    "    Compute differences between residual and node dependant threshold\n",
    "    for all residuals with negative label.\n",
    "    \n",
    "    Input:\n",
    "    X_train:            dataframe of training (pressure) residual inputs \n",
    "                        of shape samples x sensors\n",
    "    y_train:            dataframe of training (leak y/n) labels \n",
    "                        of shape samples x 1\n",
    "    thresholds:         dictionary with keys:node, values:(trained) threshold\n",
    "    \n",
    "    Output:\n",
    "    D_train             dataframe of differences between residual and threshold \n",
    "                        of shape samples x sensors\n",
    "    \"\"\"\n",
    "    \n",
    "    D_train = X_train.copy()\n",
    "    \n",
    "    # subtract sensor-dependant thresholds of each residual per sensor node\n",
    "    for sensor in thresholds.keys():\n",
    "        subtract_threshold = lambda x: x - thresholds[sensor]\n",
    "        D_train[sensor] = X_train[sensor].apply(subtract_threshold)\n",
    "    \n",
    "    # only keep samples with negative label\n",
    "    filter_noleak = y_train['y'] == 0\n",
    "    D_train = D_train[filter_noleak]\n",
    "        \n",
    "    return D_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b05caf4-76e8-49ad-8ff1-681a0c1a57d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_b_sigmoid(diff,\n",
    "                   dim,\n",
    "                   sum_threshold,\n",
    "                   b_sigmoid):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function used to estimate the required size of the sigmoid-hyperparameter\n",
    "    based on characteristics of the training data.\n",
    "    \n",
    "    Input:\n",
    "    diff:            float of a representable difference between residual\n",
    "                     and threshold of training samples with negative label\n",
    "    dim:             float of the dimension of sensor values,\n",
    "                     i.e., number of sensor nodes\n",
    "    sum_threshold:   float of sum threshold\n",
    "    b_sigmoid:       float of b in sigmoid function\n",
    "    \"\"\"\n",
    "    \n",
    "    print('T/d:        ', sum_threshold/dim)\n",
    "    print('sgd_b(diff):', sigmoid(diff,b_sigmoid))\n",
    "    print('b large enough:', sigmoid(diff,b_sigmoid) < sum_threshold/dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453a345b-2085-4031-b4d2-ddde5b471635",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Evaluation and Visualization of Results\n",
    "\n",
    "- functionality to filter data according to leak size\n",
    "- functionality to evaluate a trained multi classification resp. ensemble classification model with respect to performance and fairness\n",
    "- functionality to visualize the performance of different trained multi classification resp. ensemble classification model with respect to performance and fairness\n",
    "- functionality to visualize the dependency on the performance score on the fairness scores\n",
    "- functionality to visualize the dependency on the performance and fairness scores on the training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bfae68-02ee-4a99-a53a-9c3d48f4c47f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_diameter_Hanoi(X_clas,\n",
    "                          y_clas,\n",
    "                          X_sen,\n",
    "                          diameter,\n",
    "                          df_information,\n",
    "                          random_state=1):\n",
    "    \n",
    "    # find times where leak of diameter is fixed\n",
    "    filter_start = (df_information['node ID']=='2') & (df_information['diameter']==diameter)\n",
    "    filter_end = (df_information['node ID']=='32') & (df_information['diameter']==diameter)\n",
    "    index_start = list(df_information[filter_start].index)[0]\n",
    "    index_end = list(df_information[filter_end].index)[0]\n",
    "    time_start_diameter = df_information.loc[index_start, 'setting start ID']\n",
    "    time_end_diameter = df_information.loc[index_end, 'setting end ID'] \n",
    "\n",
    "    split = train_test_split(X_clas.loc[time_start_diameter:time_end_diameter,:], \n",
    "                             y_clas.loc[time_start_diameter:time_end_diameter,:], \n",
    "                             train_size=0.4,\n",
    "                             test_size=0.6, \n",
    "                             shuffle=True,\n",
    "                             random_state=random_state, \n",
    "                             stratify=y_clas.loc[time_start_diameter:time_end_diameter,:])\n",
    "    X_clas_train = split[0]\n",
    "    X_clas_test = split[1]\n",
    "    y_clas_train = split[2]\n",
    "    y_clas_test = split[3]\n",
    "    X_sen_train = X_sen.loc[X_clas_train.index,:]\n",
    "    X_sen_test = X_sen.loc[X_clas_test.index,:]\n",
    "    \n",
    "    return X_clas_train, X_clas_test, y_clas_train, y_clas_test, X_sen_train, X_sen_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27519101-d0c1-4c86-aaa1-5527b3ed3712",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_diameter_LTown(X_clas,\n",
    "                          y_clas,\n",
    "                          X_sen,\n",
    "                          diameter,\n",
    "                          df_leaks,\n",
    "                          random_state=1):\n",
    "    \n",
    "    # find times where leak of diameter is fixed\n",
    "    filter_diameter = df_leaks['dia'] == diameter \n",
    "\n",
    "    split = train_test_split(X_clas[filter_diameter], \n",
    "                             y_clas[filter_diameter], \n",
    "                             train_size=0.4,\n",
    "                             test_size=0.6, \n",
    "                             shuffle=True,\n",
    "                             random_state=random_state, \n",
    "                             stratify=y_clas[filter_diameter])\n",
    "    X_clas_train = split[0]\n",
    "    X_clas_test = split[1]\n",
    "    y_clas_train = split[2]\n",
    "    y_clas_test = split[3]\n",
    "    X_sen_train = X_sen.loc[X_clas_train.index,:]\n",
    "    X_sen_test = X_sen.loc[X_clas_test.index,:]\n",
    "    \n",
    "    return X_clas_train, X_clas_test, y_clas_train, y_clas_test, X_sen_train, X_sen_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af7172b-49de-45dd-a4c5-08c2ee680f5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model_clas):\n",
    "    \n",
    "    \"\"\"\n",
    "    Inputs:   \n",
    "    model_clas:   trained instance of one of the siblings of the \n",
    "                  EnsembleThresholdClassification class\n",
    "              \n",
    "    Outputs:\n",
    "    acc:          float of test accuracy score\n",
    "    eo:           float of test equal opportunity score\n",
    "    di:           float of test disparate impact score\n",
    "    TPRs:         dict with key:sensitive group, \n",
    "                  value:test TPR over all samples \n",
    "    \"\"\"\n",
    "    \n",
    "    tpr,_,fpr,_,_,acc,_ = model_clas.score(X_clas_train, \n",
    "                                           y_clas_train,\n",
    "                                           print_all_scores=False)\n",
    "    print('Train TPR:', tpr)\n",
    "    print('Train FPR:', fpr)\n",
    "    print('Train ACC:', acc)\n",
    "    \n",
    "    tpr,_,fpr,_,_,acc,_ = model_clas.score(X_clas_test, \n",
    "                                           y_clas_test,\n",
    "                                           print_all_scores=False)\n",
    "    print('Test TPR:', tpr)\n",
    "    print('Test FPR:', fpr)\n",
    "    print('Test ACC:', acc)\n",
    "    \n",
    "    TPRs,eo,di = model_clas.score_fairness(X_clas_test,\n",
    "                                           X_sen_test,\n",
    "                                           y_clas_test,\n",
    "                                           print_all_scores=False)\n",
    "    print('Test EO (small):', eo)\n",
    "    print('Test DI (large):', di)\n",
    "    \n",
    "    return acc, eo, di, TPRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa723b8a-b88e-4c15-b95a-63b14edeeaa9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def graphics_bars(results,\n",
    "                  rotate=False,\n",
    "                  title=True,\n",
    "                  save_figs_d=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    results:     dictionary of the structure\n",
    "                 {model class: {'acc':  float of accuracy,\n",
    "                              'eo':   float of equal oppotunity,\n",
    "                              'di':   float of disparate impact,\n",
    "                              'TPRs': {sensitive feature: float of TPR of sensitive group\n",
    "                                      }\n",
    "                              }\n",
    "                 }\n",
    "    rotate:      boolean to determine whether to rotate x-axis labels\n",
    "    title:       boolean to determine qhether to display title\n",
    "    save_figs_d: string or float to determine whether and under which name\n",
    "                 the created grahps should be saved\n",
    "    \"\"\"\n",
    "\n",
    "    # --- create dataframes which will be used for plotting\n",
    "    columns1 = ['method','model','ACC', 'EO','DI','group', 'TPR']\n",
    "    df1 = pd.DataFrame(data=dict(), columns=columns1)\n",
    "    columns2 = ['method', 'score type', 'ACC resp. EO']\n",
    "    df2 = pd.DataFrame(data=dict(), columns=columns2)\n",
    "    columns3 = ['method', 'score type', 'ACC resp. DI']\n",
    "    df3 = pd.DataFrame(data=dict(), columns=columns3)\n",
    "    idx1, idx2, idx3 = 0, 0, 0\n",
    "    for model_clas in results:\n",
    "        \n",
    "        sensitive_features = results[model_clas]['TPRs'].keys()\n",
    "        for sensitive_feature in sensitive_features:\n",
    "            df1.loc[idx1,'method'] = model_clas.alias\n",
    "            df1.loc[idx1,'model'] = model_clas\n",
    "            df1.loc[idx1,'ACC'] = results[model_clas]['acc']\n",
    "            df1.loc[idx1,'EO'] = results[model_clas]['eo']\n",
    "            df1.loc[idx1,'DI'] = results[model_clas]['di']\n",
    "            df1.loc[idx1,'group'] = sensitive_feature[2:]\n",
    "            df1.loc[idx1,'TPR'] = results[model_clas]['TPRs'][sensitive_feature]\n",
    "            idx1 += 1\n",
    "            \n",
    "        df2.loc[idx2,'method'] = model_clas.alias\n",
    "        df2.loc[idx2,'score type'] = 'accuracy'\n",
    "        df2.loc[idx2,'ACC resp. EO'] = results[model_clas]['acc']\n",
    "        idx2 +=1\n",
    "        df2.loc[idx2,'method'] = model_clas.alias\n",
    "        df2.loc[idx2,'score type'] = 'equal opportunity'\n",
    "        df2.loc[idx2,'ACC resp. EO'] = results[model_clas]['eo']\n",
    "        idx2 +=1\n",
    "        \n",
    "        df3.loc[idx3,'method'] = model_clas.alias\n",
    "        df3.loc[idx3,'score type'] = 'accuracy'\n",
    "        df3.loc[idx3,'ACC resp. DI'] = results[model_clas]['acc']\n",
    "        idx3 +=1\n",
    "        df3.loc[idx3,'method'] = model_clas.alias\n",
    "        df3.loc[idx3,'score type'] = 'disparate impact'\n",
    "        df3.loc[idx3,'ACC resp. DI'] = results[model_clas]['di']\n",
    "        idx3 +=1\n",
    "    \n",
    "    # --- plot bar plots of TPR, ACC, EO and DI\n",
    "    # choose seaborn color palette\n",
    "    color_map = cm.get_cmap(name='Blues', lut=1000)\n",
    "    sub_color_map = color_map(np.linspace(0.2, 0.9, 3))\n",
    "    palette = sns.color_palette(palette=sub_color_map)\n",
    "    \n",
    "    # true positive rate per method and sensitive group\n",
    "    if rotate:\n",
    "        plt.figure(figsize=(5,3.6))\n",
    "    else:\n",
    "        plt.figure(figsize=(5,3))\n",
    "    sns.barplot(x='method', y='TPR', \n",
    "                hue='group', \n",
    "                data=df1, \n",
    "                palette=palette,\n",
    "                estimator=lambda x: np.mean(x),\n",
    "                errorbar=lambda x: (x.min(),x.max()))\n",
    "    plt.legend(loc='lower left', bbox_to_anchor=(0.0, 0.0),\n",
    "              fontsize='medium')\n",
    "    if title:\n",
    "        plt.title('TPR per method and per sensitive group (d={})'.format(save_figs_d),\n",
    "              fontsize='medium')\n",
    "    else:\n",
    "        plt.title(' ')\n",
    "    if rotate:\n",
    "        plt.xticks(rotation=30)\n",
    "    plt.tight_layout()\n",
    "    if save_figs_d!=None:\n",
    "        plt.savefig('TPRperMethodAndGroup_d{}.pdf'.format(save_figs_d),\n",
    "                    format='pdf')\n",
    "    plt.show()\n",
    "    \n",
    "    # accuracy and equal opportunity per method\n",
    "    if rotate:\n",
    "        plt.figure(figsize=(5,3.6))\n",
    "    else:\n",
    "        plt.figure(figsize=(5,3))\n",
    "    sns.barplot(x='method', y='ACC resp. EO', \n",
    "                hue='score type', \n",
    "                data=df2, \n",
    "                palette=palette,\n",
    "                estimator=lambda x: np.mean(x),\n",
    "                errorbar=lambda x: (x.min(),x.max()))\n",
    "    plt.ylabel('accuracy and equal opportunity',\n",
    "              fontsize='medium')\n",
    "    plt.legend(loc='lower left', bbox_to_anchor=(0.0, 0.0),\n",
    "              fontsize='medium')\n",
    "    if title:\n",
    "        plt.title('Accuracy and equal opportunity per method (d={})'.format(save_figs_d),\n",
    "                  fontsize='medium')\n",
    "    else:\n",
    "        plt.title(' ')\n",
    "    if rotate:\n",
    "        plt.xticks(rotation=30)\n",
    "    plt.tight_layout()\n",
    "    if save_figs_d!=None:\n",
    "        plt.savefig('ACCandEOperMethod_d{}.pdf'.format(save_figs_d),\n",
    "                    format='pdf')\n",
    "    plt.show()\n",
    "    \n",
    "     # accuracy and disparate impact per method\n",
    "    if rotate:\n",
    "        fig = plt.figure(figsize=(5,3.6))\n",
    "    else:\n",
    "        fig = plt.figure(figsize=(5,3))\n",
    "    sns.barplot(x='method', y='ACC resp. DI', \n",
    "                hue='score type', \n",
    "                data=df3, \n",
    "                palette=palette,\n",
    "                estimator=lambda x: np.mean(x),\n",
    "                errorbar=lambda x: (x.min(),x.max()))\n",
    "    plt.ylabel('accuracy and disparate impact',\n",
    "              fontsize='medium')\n",
    "    plt.legend(loc='lower left', bbox_to_anchor=(0.0, 0.0),\n",
    "              fontsize='medium')\n",
    "    if title:\n",
    "        plt.title('Accuracy and disparate impact per method (d={})'.format(save_figs_d),\n",
    "                  fontsize='medium')\n",
    "    else:\n",
    "        plt.title(' ')\n",
    "    if rotate:\n",
    "        plt.xticks(rotation=30)\n",
    "    plt.tight_layout()\n",
    "    if save_figs_d!=None:\n",
    "        plt.savefig('ACCandDIperMethod_d{}.pdf'.format(save_figs_d),\n",
    "                    format='pdf')\n",
    "    plt.show()\n",
    "    \n",
    "    return df1, df2, df3, fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba39d736-6c53-459c-9228-b8431edb0ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphics_scatter(results_fairness,\n",
    "                     results_nofairness,\n",
    "                     comparisons,\n",
    "                     title=True,\n",
    "                     horizontal=True,\n",
    "                     save_figs=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    results_fairness:   dictionary of the structure\n",
    "                        {model alias: {diameter: {'ACCs': list of accuracies,\n",
    "                                                  'EOs':  list of equal opportunities,\n",
    "                                                  'DIs':  list of disparate impacts,\n",
    "                                                  'Cs':   list of hyperparameters\n",
    "                                                  }\n",
    "                                       }\n",
    "                         }\n",
    "    results_nofairness: dictionary of the structure\n",
    "                        {model alias: {diameter: {'acc': float of accuracy,\n",
    "                                                  'eo':  float of equal opportunity,\n",
    "                                                  'di':  float of disparate impact,\n",
    "                                                  'c':   float of hyperparameter\n",
    "                                                  }\n",
    "                                       }\n",
    "                         }\n",
    "    comparisons:         dictionary with key:fairness algorithm alias,\n",
    "                         value:comparison algorithm alias\n",
    "    title:               boolean to determine whether to display title\n",
    "    horizontal:          boolean to determine whether to obtain a 3x1 or 1x3 plot\n",
    "    save_figs:           boolean to determine whether creates grahps should be saved\n",
    "    \"\"\"\n",
    "    \n",
    "    # choose matplotlib color palette\n",
    "    nb_colors = len(list(results_fairness[list(results_fairness.keys())[0]].keys()))\n",
    "    color_map = cm.get_cmap(name='Blues', lut=1000)\n",
    "    sub_color_map = color_map(np.linspace(0.9, 0.2, nb_colors))\n",
    "            \n",
    "    # --- plot scatter plots for coherence of accuracy and equal opportunity  \n",
    "    if horizontal:\n",
    "        plt.figure(figsize=(18,5)) \n",
    "    else:\n",
    "        plt.figure(figsize=(5,12))\n",
    "        \n",
    "    for i,diameter in enumerate(results_fairness.keys()):\n",
    "        \n",
    "        if horizontal:\n",
    "            plt.subplot(1,3,i+1)\n",
    "        else:\n",
    "            plt.subplot(3,1,i+1)\n",
    "        for j,(color,alias) in enumerate(zip(sub_color_map,results_fairness[diameter].keys())):\n",
    "            \n",
    "            # access fairness results\n",
    "            ACCs = results_fairness[diameter][alias]['ACCs'].copy()\n",
    "            EOs = results_fairness[diameter][alias]['EOs'].copy()\n",
    "            # shift the first entry, corresponding to the value (0,0.5)\n",
    "            # a little bit as else, the point would overlap perfectly\n",
    "            EOs[0] = EOs[0]+j*0.0025\n",
    "            \n",
    "            # access comparison (non-fairness) results\n",
    "            acc = results_nofairness[diameter][comparisons[alias]]['acc']\n",
    "            eo = results_nofairness[diameter][comparisons[alias]]['eo']\n",
    "            \n",
    "            # accuracy vs. equal opportunity\n",
    "            plt.scatter([eo],[acc], color=color, marker='x')\n",
    "            plt.scatter(EOs, ACCs, \n",
    "                        color=color,\n",
    "                        label='d={}, method={}'.format(diameter,\n",
    "                                                       alias))\n",
    "        if horizontal:\n",
    "            size1 = 'large'\n",
    "            size2 = 'x-large'\n",
    "            x_pos = 0.51\n",
    "            y_pos = 0.99\n",
    "        else:\n",
    "            size1 = 'medium'\n",
    "            size2 = 'large'\n",
    "            x_pos = 0.55\n",
    "            y_pos = 0.99\n",
    "            \n",
    "        plt.xlabel('equal opportunity',\n",
    "                   fontsize=size1)\n",
    "        plt.ylabel('accuracy',\n",
    "                   fontsize=size1)\n",
    "        plt.legend(fontsize=size1,\n",
    "                   loc='lower right')\n",
    "    \n",
    "    if title:\n",
    "        plt.suptitle('Coherence of accuracy and equal opportunity',\n",
    "                     fontsize=size2, x=x_pos, y=y_pos)\n",
    "    else:\n",
    "        plt.suptitle(' ',\n",
    "                     fontsize=size2, x=x_pos, y=y_pos)\n",
    "    plt.tight_layout()\n",
    "    if save_figs:\n",
    "        plt.savefig('ACCandEOCoherence.pdf',\n",
    "                    format='pdf')\n",
    "    plt.show()\n",
    "    \n",
    "    # --- plot scatter plots for coherence of accuracy and disparate impact\n",
    "    if horizontal:\n",
    "        plt.figure(figsize=(18,5)) \n",
    "    else:\n",
    "        plt.figure(figsize=(5,12))\n",
    "        \n",
    "    for i,diameter in enumerate(results_fairness.keys()):\n",
    "        \n",
    "        if horizontal:\n",
    "            plt.subplot(1,3,i+1) # remark: three diameters are assumed here\n",
    "        else:\n",
    "            plt.subplot(3,1,i+1) # remark: three diameters are assumed here\n",
    "        for j,(color,alias) in enumerate(zip(sub_color_map,results_fairness[diameter].keys())):\n",
    "            \n",
    "            # access fairness results\n",
    "            ACCs = results_fairness[diameter][alias]['ACCs'].copy()\n",
    "            DIs = results_fairness[diameter][alias]['DIs'].copy()\n",
    "            # shift the first entry, corresponding to the value (1,0.5)\n",
    "            # a little bit as else, the point would overlap perfectly\n",
    "            DIs[0] = DIs[0]-j*0.0025\n",
    "            \n",
    "            # access comparison (non-fairness) results\n",
    "            acc = results_nofairness[diameter][comparisons[alias]]['acc']\n",
    "            di = results_nofairness[diameter][comparisons[alias]]['di']\n",
    "            \n",
    "            # accuracy vs. disparate impact\n",
    "            plt.scatter([di],[acc], color=color, marker='x')\n",
    "            plt.scatter(DIs, ACCs, \n",
    "                        color=color,\n",
    "                        #s=50/(j+1), # changes size in case of overlapping datapoints                                                            \n",
    "                        label='d={}, method={}'.format(diameter,\n",
    "                                                       alias))\n",
    "        if horizontal:\n",
    "            size1 = 'large'\n",
    "            size2 = 'x-large'\n",
    "            x_pos = 0.52\n",
    "            y_pos = 0.99\n",
    "        else:\n",
    "            size1 = 'medium'\n",
    "            size2 = 'large'\n",
    "            x_pos = 0.56\n",
    "            y_pos = 0.99\n",
    "        \n",
    "        plt.xlabel('disparate impact',\n",
    "                   fontsize=size1) \n",
    "        plt.ylabel('accuracy', \n",
    "                   fontsize=size1)\n",
    "        plt.legend(fontsize=size1, \n",
    "                   loc='lower left')\n",
    "    \n",
    "    if title:\n",
    "        plt.suptitle('Coherence of accuracy and disparate impact',\n",
    "                     fontsize=size2, x=x_pos, y=y_pos)\n",
    "    else:\n",
    "         plt.suptitle(' ',\n",
    "                 fontsize=size2, x=x_pos, y=y_pos)\n",
    "    plt.tight_layout()\n",
    "    if save_figs:\n",
    "        plt.savefig('ACCandDICoherence.pdf',\n",
    "                    format='pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a43628-d3a5-4b0a-a9fc-96a6eead20c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphics_lines(results_fairness,\n",
    "                   results_nofairness,\n",
    "                   comparisons,\n",
    "                   columns='methods',\n",
    "                   with_eo=False,\n",
    "                   save_figs=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    results_fairness:   dictionary of the structure\n",
    "                        {model alias: {diameter: {'ACCs': list of accuracies,\n",
    "                                                  'EOs':  list of equal opportunities,\n",
    "                                                  'DIs':  list of disparate impacts,\n",
    "                                                  'Cs':   list of hyperparameters\n",
    "                                                  }\n",
    "                                       }\n",
    "                        }\n",
    "    results_nofairness: dictionary of the structure\n",
    "                        {model alias: {diameter: {'acc': float of accuracy,\n",
    "                                                  'eo':  float of equal opportunity,\n",
    "                                                  'di':  float of disparate impact,\n",
    "                                                  'c':   float of hyperparameter\n",
    "                                                  }\n",
    "                                       }\n",
    "                         }\n",
    "    comparisons:         dictionary with key:fairness algorithm alias,\n",
    "                         value:comparison algorithm alias\n",
    "    columns:             string of either 'methods' or 'diameters' to determine if the plot is\n",
    "                         of size nb_diameters x nb_methods (columns='methods') or\n",
    "                         of size nb_methods x nb_diameters (columns='diameters')\n",
    "    with_eo:             boolean to determine whether \n",
    "                         the equal opportunity should be plotted\n",
    "    save_figs:           boolean to determine whether \n",
    "                         created grahps should be saved\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- plot line plots for coherence of \n",
    "    # --- accuracy, equal opportunity and disparate impact, resp.,\n",
    "    # --- and the hyperparameter\n",
    "    \n",
    "    # choose matplotlib color palette\n",
    "    color_map = cm.get_cmap(name='Blues', lut=1000)\n",
    "    sub_color_map = color_map(np.linspace(0.9, 0.2, 3))\n",
    "    \n",
    "    if with_eo:\n",
    "        plt.figure(figsize=(20,25)) # remark: this could be optimized wrt. nb_diameters,\n",
    "                                    #         nb_methods and the columns variable.\n",
    "    else:    \n",
    "        plt.figure(figsize=(20,15)) # remark: this could be optimized wrt. nb_diameters,\n",
    "                                    #         nb_methods and the columns variable.\n",
    "    \n",
    "    nb_diameters = len(list(results_fairness.keys()))\n",
    "    nb_methods = len(list(results_fairness[list(results_fairness.keys())[0]].keys()))\n",
    "    for i, diameter in enumerate(results_fairness.keys()):\n",
    "    \n",
    "        for j, alias in enumerate(results_fairness[diameter].keys()):\n",
    "            \n",
    "            # access fairness results\n",
    "            ACCs = results_fairness[diameter][alias]['ACCs']\n",
    "            EOs = results_fairness[diameter][alias]['EOs']\n",
    "            DIs = results_fairness[diameter][alias]['DIs']\n",
    "            if 'Cs' in list(results_fairness[diameter][alias].keys()):\n",
    "                Cs = results_fairness[diameter][alias]['Cs']\n",
    "                hyperparameter = 'covariance hyperparameter'\n",
    "            if 'Lambdas' in list(results_fairness[diameter][alias].keys()):\n",
    "                Cs = results_fairness[diameter][alias]['Lambdas']\n",
    "                hyperparameter = 'lambda hyperparameter'\n",
    "                \n",
    "            if columns=='methods':\n",
    "                plt.subplot(nb_diameters,nb_methods,(j+1)+i*nb_methods)\n",
    "            if columns=='diameters':\n",
    "                plt.subplot(nb_methods,nb_diameters,(i+1)+j*nb_diameters)\n",
    "            \n",
    "            # accuracy vs. hyperparameter\n",
    "            plt.plot(Cs, ACCs, \n",
    "                     color=sub_color_map[0],\n",
    "                     label='accuracy')\n",
    "            plt.scatter(Cs, ACCs, \n",
    "                        color=sub_color_map[0],\n",
    "                        marker='x')\n",
    "            \n",
    "            # equal opportunity vs. hyperparameter\n",
    "            if with_eo:\n",
    "                plt.plot(Cs, EOs, \n",
    "                         color=sub_color_map[2],\n",
    "                         label='equal opportunity')\n",
    "                plt.scatter(Cs, EOs, \n",
    "                             color=sub_color_map[2],\n",
    "                             marker='x')\n",
    "            \n",
    "            # disparate impact vs. hyperparameter\n",
    "            plt.plot(Cs, DIs, \n",
    "                    color=sub_color_map[1],\n",
    "                     label='disparate impact')\n",
    "            plt.scatter(Cs, DIs, \n",
    "                        color=sub_color_map[1],\n",
    "                        marker='x')\n",
    "            \n",
    "            plt.xticks(rotation=45)\n",
    "            plt.xlabel(hyperparameter,\n",
    "                       fontsize='large')\n",
    "            if with_eo:\n",
    "                plt.ylabel('accuracy, equal opportunity and disparate impact',\n",
    "                           fontsize='large')\n",
    "            else:\n",
    "                plt.ylabel('accuracy and disparate impact',\n",
    "                           fontsize='large')\n",
    "            plt.legend(fontsize='large')\n",
    "            plt.title('d={}, method={}'.format(diameter,\n",
    "                                               alias),\n",
    "                      fontsize='large')\n",
    "    \n",
    "    if with_eo:\n",
    "        plt.suptitle('Accuracy, equal opportunity and disparate impact\\n'\\\n",
    "                     'in dependence on the method and its hyperparameter',\n",
    "                     fontsize='x-large',\n",
    "                     x=0.52, y=0.99)\n",
    "        plt.tight_layout()\n",
    "        if save_figs:\n",
    "            plt.savefig('ACCandEOandDIvsHyperparameterCoherence.pdf',\n",
    "                        format='pdf')\n",
    "    else:\n",
    "        plt.suptitle('Accuracy and disparate impact\\n'\\\n",
    "                     'in dependence on the method and its hyperparameter', \n",
    "                     fontsize='x-large',\n",
    "                     x=0.52, y=0.99)\n",
    "        plt.tight_layout()\n",
    "        if save_figs:\n",
    "            plt.savefig('ACCandDIvsHyperparameterCoherence.pdf',\n",
    "                        format='pdf')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
